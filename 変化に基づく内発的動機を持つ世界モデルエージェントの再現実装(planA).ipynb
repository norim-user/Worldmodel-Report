{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSKjG29n3OdEIda51JGvYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/norim-user/Worldmodel-Report/blob/main/%E5%A4%89%E5%8C%96%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E5%86%85%E7%99%BA%E7%9A%84%E5%8B%95%E6%A9%9F%E3%82%92%E6%8C%81%E3%81%A4%E4%B8%96%E7%95%8C%E3%83%A2%E3%83%87%E3%83%AB%E3%82%A8%E3%83%BC%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%88%E3%81%AE%E5%86%8D%E7%8F%BE%E5%AE%9F%E8%A3%85(planA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "実装論文:変化に基づく内発的動機を持つ世界モデルエージェント,https://arxiv.org/abs/2503.21047"
      ],
      "metadata": {
        "id": "G0u_Haj_DVjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Dive: Plan A\n",
        "\n",
        "このプランでは、「AIが夢を見る」仕組み（VAEとRNNの連携）を深く理解するために、DreamerV3の構造（RSSM, Encoder, Decoder, Actor, Critic）をすべてPyTorchで実装します。ただし、Colabで動かすために極限まで小型化させます。\n",
        "\n",
        "実行内容\n",
        "\n",
        "VRAMが2倍必要という問題を、モデルのサイズを1/10以下にした。\n",
        "\n",
        "CNN（画像処理）の層を減らし、潜在空間（Latent Space）のサイズを小さくした。"
      ],
      "metadata": {
        "id": "HW0d3ZmUwpgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Install Dependencies\n",
        "Minigridと、可視化用のライブラリをインストールします。"
      ],
      "metadata": {
        "id": "PdBxG7_uDfpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR4blaKtDPAy",
        "outputId": "bbfb19a6-62a5-4dc3-97dc-a196e2ceec24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minigrid\n",
            "  Downloading minigrid-3.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu128)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from minigrid) (1.2.3)\n",
            "Requirement already satisfied: pygame>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.6.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym) (3.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym) (0.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (0.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading minigrid-3.0.0-py3-none-any.whl (136 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: minigrid\n",
            "Successfully installed minigrid-3.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install Minigrid and dependencies\n",
        "!pip install minigrid gym matplotlib torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Verify Environment (The \"Unlock\" Task)\n",
        "論文でタスク環境として使用されている 'MiniGrid-Unlock-v0' (あるいはそれに準ずる環境) をロードし、エージェントの視界（Observation）を確認"
      ],
      "metadata": {
        "id": "-Vlgv4ZzDxY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym # Minigrid now uses gymnasium\n",
        "import minigrid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 論文で使用されている \"Unlock\" 環境 (Task Environment)\n",
        "# ※最新のMinigridでは名称が少し異なる場合がありますが、基本構造は同じです\n",
        "env_name = 'MiniGrid-Unlock-v0'\n",
        "\n",
        "try:\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "except gym.error.Error:\n",
        "    # フォールバック: 標準的な環境を使用\n",
        "    print(f\"{env_name} not found, using MiniGrid-DoorKey-5x5-v0\")\n",
        "    env = gym.make('MiniGrid-DoorKey-5x5-v0', render_mode='rgb_array')\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "# --- Visualization ---\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(env.render())\n",
        "plt.title(f\"Environment: {env.spec.id}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Check Observation Shape\n",
        "# DreamerV3は画像をそのまま入力します\n",
        "print(f\"Observation keys: {observation.keys()}\")\n",
        "print(f\"Image shape: {observation['image'].shape}\") # 通常 (7, 7, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "SqWfaiN9Dv91",
        "outputId": "c01c35f5-5134-4c9d-ba6c-7f9c0ee1b2e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
            "Image shape: (7, 7, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2: Implementing CBET Logic (The Intrinsic Reward)環境構築と並行して、この論文の魂である CBET (Change-Based Exploration Transfer) の核となる数式を実装しましょう。これはGPUを使わない純粋なロジックなので、Colab Freeでも実装可能です。論文の Equation 1  をコードに変換します。$$r_{i}(s)=\\frac{1}{n(s)+n(c)}$$$r_i(s)$: 好奇心報酬（Intrinsic Reward）\n",
        "\n",
        "$n(s)$: その状態 $s$ を訪れた回数（Visits count）\n",
        "\n",
        "$n(c)$: 変化 $c$ が起きた回数（Change count）\n",
        "\n",
        "$c$: 状態の変化（$s' - s$）"
      ],
      "metadata": {
        "id": "zlOvqOYBEHz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step3 :CBET Module Implementation (PyTorch)"
      ],
      "metadata": {
        "id": "j9U509lUFQXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import hashlib\n",
        "\n",
        "class CBETReward:\n",
        "    def __init__(self, decay=0.99):\n",
        "        \"\"\"\n",
        "        論文に基づくCBETの実装。\n",
        "        Args:\n",
        "            decay: 論文では言及が少ないが、カウントを無限に増やさないための減衰率(任意)\n",
        "                   論文では random reset を採用している [cite: 61]\n",
        "        \"\"\"\n",
        "        self.state_counts = {}  # n(s)\n",
        "        self.change_counts = {} # n(c)\n",
        "\n",
        "    def get_state_hash(self, state_tensor):\n",
        "        \"\"\"\n",
        "        高次元の状態をハッシュ化して辞書のキーにする。\n",
        "        論文ではSimHashなどが使われるが、ここでは簡易的に文字列化またはtostringを使用。\n",
        "        Minigridのような離散的な状態ならこれで十分。\n",
        "        \"\"\"\n",
        "        # 簡易実装: tensorをバイト列にしてハッシュ化\n",
        "        return hashlib.md5(state_tensor.cpu().numpy().tobytes()).hexdigest()\n",
        "\n",
        "    def compute_reward(self, current_state, next_state):\n",
        "        \"\"\"\n",
        "        Eq. 1 に基づく報酬計算\n",
        "        \"\"\"\n",
        "        # 1. 状態のハッシュ化\n",
        "        s_hash = self.get_state_hash(current_state)\n",
        "\n",
        "        # 2. 変化(Change)の計算: c = s' - s\n",
        "        # 画像ベースの場合、ピクセル差分をとる\n",
        "        change = next_state - current_state\n",
        "        c_hash = self.get_state_hash(change)\n",
        "\n",
        "        # 3. カウント更新\n",
        "        self.state_counts[s_hash] = self.state_counts.get(s_hash, 0) + 1\n",
        "        self.change_counts[c_hash] = self.change_counts.get(c_hash, 0) + 1\n",
        "\n",
        "        n_s = self.state_counts[s_hash]\n",
        "        n_c = self.change_counts[c_hash]\n",
        "\n",
        "        # 4. Intrinsic Reward 計算\n",
        "        r_i = 1.0 / (n_s + n_c)\n",
        "\n",
        "        return r_i\n",
        "\n",
        "# --- Test the Module ---\n",
        "# ダミーデータでテスト\n",
        "cbet = CBETReward()\n",
        "state_t = torch.zeros((3, 7, 7)) # ダミー画像\n",
        "state_t1 = state_t.clone()\n",
        "state_t1[0, 1, 1] = 1.0 # 少し変化させる\n",
        "\n",
        "reward = cbet.compute_reward(state_t, state_t1)\n",
        "print(f\"Intrinsic Reward for first change: {reward}\") # 期待値: 1/(1+1) = 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRl1c4_WEDZV",
        "outputId": "55271921-2c7d-4e68-a55f-d85a55d67bc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intrinsic Reward for first change: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step4: The \"Dual-Doctor\" Logic (Eq. 3 Implementation)\n",
        "この論文の著者は、DreamerV3の内部構造を改造する困難さを避けるため、2つの独立したDreamerV3を並列に走らせ、その出力を混ぜるという解決策を取りました 。数式を確認しましょう：$$\\pi_{TASK}(x, a) = \\sigma(f_i(w_i(x), a) + f_e(w_e(x), a))$$これをコード（Python class）に落とし込みます。"
      ],
      "metadata": {
        "id": "frkByGktHtXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation Code (The Wrapper)\n",
        "\n",
        "どんなAIモデルであっても「2つ混ぜて動かす」ことができるようにするラッパー"
      ],
      "metadata": {
        "id": "Uk5ZMnnNHtT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. The Mock Agent (ダミーの脳) ---\n",
        "# まずは「脳」の代わりになる簡単なクラスを作ります。\n",
        "# 本番ではここが「DreamerV3」に置き換わります。\n",
        "class MockDreamerAgent(nn.Module):\n",
        "    def __init__(self, action_dim):\n",
        "        super().__init__()\n",
        "        self.action_dim = action_dim\n",
        "        # ダミーの重み（ランダムな思考を出力するため）\n",
        "        self.dummy_layer = nn.Linear(10, action_dim)\n",
        "\n",
        "    def get_logits(self, observation):\n",
        "        \"\"\"\n",
        "        観察(画像)を受け取り、行動の優先度(Logits)を返す\n",
        "        \"\"\"\n",
        "        # 本来はここで画像をVAEで圧縮し、RNNで未来を予測する\n",
        "        # ここではダミーとしてランダムな数値を作成\n",
        "        batch_size = 1\n",
        "        dummy_feature = torch.randn(batch_size, 10)\n",
        "        logits = self.dummy_layer(dummy_feature)\n",
        "        return logits\n",
        "\n",
        "# --- 2. The Dual-Doctor Wrapper (論文の核心: Eq. 3) ---\n",
        "class DualModelPolicy(nn.Module):\n",
        "    def __init__(self, explorer_agent, worker_agent):\n",
        "        super().__init__()\n",
        "        self.explorer = explorer_agent # f_i (Intrinsic)\n",
        "        self.worker = worker_agent     # f_e (Extrinsic)\n",
        "\n",
        "    def get_action(self, observation):\n",
        "        # Step 1: 探索担当の脳に意見を聞く (f_i)\n",
        "        logits_i = self.explorer.get_logits(observation)\n",
        "\n",
        "        # Step 2: 仕事担当の脳に意見を聞く (f_e)\n",
        "        logits_e = self.worker.get_logits(observation)\n",
        "\n",
        "        # Step 3: 意見を統合する (Summation)\n",
        "        # Logits（確率になる前の生数値）を足し合わせるのがポイントです\n",
        "        combined_logits = logits_i + logits_e\n",
        "\n",
        "        # Step 4: 確率に変換して行動を選ぶ (Softmax & Sample)\n",
        "        probs = F.softmax(combined_logits, dim=-1)\n",
        "\n",
        "        # 確率に従ってサイコロを振る\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        return action.item(), probs\n",
        "\n",
        "# --- Test the Architecture ---\n",
        "action_dim = 7 # Minigridの行動パターン数\n",
        "agent_i = MockDreamerAgent(action_dim) # 事前学習済みの探索エージェント (想定)\n",
        "agent_e = MockDreamerAgent(action_dim) # これから育てるタスクエージェント\n",
        "\n",
        "# 合体させる\n",
        "dual_brain = DualModelPolicy(agent_i, agent_e)\n",
        "\n",
        "# テスト実行\n",
        "dummy_obs = torch.zeros(1, 7, 7, 3) # ダミー画像\n",
        "action, probabilities = dual_brain.get_action(dummy_obs)\n",
        "\n",
        "print(f\"Selected Action Index: {action}\")\n",
        "print(f\"Combined Probabilities: {probabilities.detach().numpy()}\")\n",
        "print(\"Dual-Doctor mechanism is working.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWhXFcXHHt88",
        "outputId": "b86cc194-fea9-4d05-9a3d-2a40bc6dca06"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Action Index: 5\n",
            "Combined Probabilities: [[0.04477187 0.05482612 0.08011986 0.1425191  0.03085134 0.49385637\n",
            "  0.15305525]]\n",
            "Dual-Doctor mechanism is working.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step5以降はPlanAとPlanBで実行内容が異なる"
      ],
      "metadata": {
        "id": "OtFjp4A8h6eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Implementing the RSSM (The Engine)\n",
        "RSSMは、**「決定論的（Deterministic）」な処理と「確率的（Stochastic）」**な処理を融合させたモデルです。決定論的パス ($h_t$): RNN（GRU）を使い、文脈を記憶します。\n",
        "\n",
        "確率的パス ($z_t$): VAEを使い、世界の不確実性（複数の未来の可能性）を表現しました。\n",
        "\n",
        "以下のコードは、Colab Freeでも動くように限界までパラメータを削ぎ落とした \"Nano-RSSM\" の実装です"
      ],
      "metadata": {
        "id": "XY2SAbBTLVzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RSSM(nn.Module):\n",
        "    def __init__(self, action_dim, embed_dim=64, deter_dim=200, stoch_dim=30):\n",
        "        super().__init__()\n",
        "        self.deter_dim = deter_dim  # 決定論的状態 (GRUの隠れ層サイズ)\n",
        "        self.stoch_dim = stoch_dim  # 確率的状態 (潜在変数のサイズ)\n",
        "        self.embed_dim = embed_dim  # 画像圧縮ベクトルのサイズ\n",
        "\n",
        "        # 1. Recurrent Unit (過去の記憶)\n",
        "        # 入力: 直前の確率状態(z) + 行動(a) -> 出力: 次の決定論的状態(h)\n",
        "        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n",
        "\n",
        "        # 2. Prior Network (未来予測)\n",
        "        # 決定論的状態(h)だけを見て、次の確率状態(z)を推測する\n",
        "        # \"目を閉じて次を想像する\" 回路\n",
        "        self.fc_prior = nn.Linear(deter_dim, stoch_dim * 2) # 平均と分散を出すため2倍\n",
        "\n",
        "        # 3. Posterior Network (現実認識)\n",
        "        # 決定論的状態(h) + 実際の観測(embed)を見て、正しい確率状態(z)を作る\n",
        "        # \"目を開けて答え合わせをする\" 回路\n",
        "        self.fc_post = nn.Linear(deter_dim + embed_dim, stoch_dim * 2)\n",
        "\n",
        "    def initial_state(self, batch_size):\n",
        "        # 最初の状態はゼロで埋める\n",
        "        return dict(\n",
        "            mean  = torch.zeros(batch_size, self.stoch_dim),\n",
        "            std   = torch.zeros(batch_size, self.stoch_dim),\n",
        "            stoch = torch.zeros(batch_size, self.stoch_dim),\n",
        "            deter = torch.zeros(batch_size, self.deter_dim)\n",
        "        )\n",
        "\n",
        "    def observe(self, embed, prev_action, prev_state):\n",
        "        \"\"\"\n",
        "        現実の画像(embed)を見ながら状態を更新するステップ\n",
        "        \"\"\"\n",
        "        # A. GRUで記憶(h)を更新\n",
        "        # input: [前回のz, 前回の行動]\n",
        "        gru_input = torch.cat([prev_state['stoch'], prev_action], dim=1)\n",
        "        deter = self.gru(gru_input, prev_state['deter'])\n",
        "\n",
        "        # B. 事後分布(Posterior)を計算\n",
        "        # 現実の画像(embed)もヒントにする\n",
        "        x = torch.cat([deter, embed], dim=1)\n",
        "        stats = self.fc_post(x)\n",
        "        mean, std = torch.chunk(stats, 2, dim=1)\n",
        "        std = F.softplus(std) + 0.1 # 分散は正の値にする\n",
        "\n",
        "        # C. サンプリング (Reparameterization Trick)\n",
        "        # ここで \"z\" が生まれる\n",
        "        noise = torch.randn_like(mean)\n",
        "        stoch = mean + std * noise\n",
        "\n",
        "        return {'mean': mean, 'std': std, 'stoch': stoch, 'deter': deter}\n",
        "\n",
        "    def imagine(self, prev_action, prev_state):\n",
        "        \"\"\"\n",
        "        画像を見ずに、脳内だけで次を想像するステップ (Dreaming)\n",
        "        \"\"\"\n",
        "        # A. GRUで記憶(h)を更新 (Observeと同じ)\n",
        "        gru_input = torch.cat([prev_state['stoch'], prev_action], dim=1)\n",
        "        deter = self.gru(gru_input, prev_state['deter'])\n",
        "\n",
        "        # B. 事前分布(Prior)を計算\n",
        "        # 画像(embed)が無いので、記憶(h)だけで推測する\n",
        "        stats = self.fc_prior(deter)\n",
        "        mean, std = torch.chunk(stats, 2, dim=1)\n",
        "        std = F.softplus(std) + 0.1\n",
        "\n",
        "        # C. サンプリング\n",
        "        noise = torch.randn_like(mean)\n",
        "        stoch = mean + std * noise\n",
        "\n",
        "        return {'mean': mean, 'std': std, 'stoch': stoch, 'deter': deter}\n",
        "\n",
        "print(\"Nano-RSSM module defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXzm0Ek-EcZc",
        "outputId": "64a24fbc-275a-4433-f79f-5a82d775ddea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nano-RSSM module defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Encoder & Decoderの実装\n",
        "RSSMは数値ベクトルしか理解できません。 一方で、Minigridの世界は画像（ピクセル）です。 このギャップを埋めるために、EncoderとDecoderを実装しました。\n",
        "\n",
        "Colab向けに、極限まで軽量化した \"Nano-Vision\" モジュールを定義。"
      ],
      "metadata": {
        "id": "ytOVJ1YwG1p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    画像(64x64x3)を受け取り、圧縮ベクトル(embed_dim)に変換する\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim=64):\n",
        "        super().__init__()\n",
        "        # 軽量化のため、Conv2dの層を最小限に\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 4, stride=2), # -> 31x31\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 4, stride=2), # -> 14x14\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2), # -> 6x6\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        # 最終的に embed_dim (64次元) に圧縮\n",
        "        self.fc = nn.Linear(64 * 5 * 5, embed_dim) # Minigridのサイズに合わせて調整\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # obs: (Batch, C, H, W)\n",
        "        x = self.conv(obs)\n",
        "        return self.fc(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    決定論的状態(deter)と確率状態(stoch)を受け取り、元の画像を復元する\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim=64, deter_dim=200, stoch_dim=30):\n",
        "        super().__init__()\n",
        "        self.input_dim = deter_dim + stoch_dim\n",
        "\n",
        "        self.fc = nn.Linear(self.input_dim, 64 * 5 * 5)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.Unflatten(1, (64, 5, 5)),\n",
        "            nn.ConvTranspose2d(64, 32, 5, stride=2), # 逆畳み込み\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, 6, stride=2),  # -> 64x64を目指す\n",
        "            # 画像なので最後はSigmoid (0~1) または Linear\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        # state: {deter, stoch, ...}\n",
        "        # 脳内の状態を結合\n",
        "        x = torch.cat([state['deter'], state['stoch']], dim=1)\n",
        "        x = self.fc(x)\n",
        "        return self.deconv(x)\n",
        "\n",
        "print(\"Nano-Vision (Encoder/Decoder) modules defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnSHNkX-GVPp",
        "outputId": "fc5e1aff-b61b-424c-fb78-24d4753abf09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nano-Vision (Encoder/Decoder) modules defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step7: The Assembly (World Model Integration)\n",
        "WorldModelクラスを作成します。\n",
        "\n",
        "1.見る (Encode): 過去の一連の画像（動画）を、圧縮ベクトルに変換。\n",
        "\n",
        "2.理解 (RSSM Loop): 時間を一歩ずつ進めながら、過去の記憶と新しい視覚情報を統合\n",
        "\n",
        "3.再構成 (Decode): 脳内の状態から「自分が見ているはずの画像」を再描画。"
      ],
      "metadata": {
        "id": "XJVrqtCm9qLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WorldModel(nn.Module):\n",
        "    def __init__(self, action_dim, lr=1e-4):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.rssm = RSSM(action_dim)\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "        # オプティマイザー（学習担当）もここに持たせます\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, obs, actions):\n",
        "        \"\"\"\n",
        "        Forward pass for Training (Imagination Rehearsal)\n",
        "        obs: (Batch, Seq_Len, Channels, Height, Width) -> 動画データ\n",
        "        actions: (Batch, Seq_Len, Action_Dim) -> 行動履歴\n",
        "        \"\"\"\n",
        "        b, t, c, h, w = obs.shape\n",
        "\n",
        "        # 1. 視覚処理 (Time-Distributed Encoding)\n",
        "        # 時間次元(t)とバッチ(b)をまとめて一気にエンコードして高速化\n",
        "        embed = self.encoder(obs.view(b * t, c, h, w))\n",
        "        embed = embed.view(b, t, -1) # 元の時系列に戻す\n",
        "\n",
        "        # 2. 記憶と予測 (RSSM Loop)\n",
        "        prev_state = self.rssm.initial_state(b)\n",
        "        states_list = []\n",
        "\n",
        "        for i in range(t):\n",
        "            # 現在のフレームの圧縮情報\n",
        "            current_embed = embed[:, i]\n",
        "            # 直前の行動 (最初のステップはゼロ埋めなどの処理が必要だが、簡略化のためそのまま使用)\n",
        "            prev_action = actions[:, i]\n",
        "\n",
        "            # 脳の状態を更新\n",
        "            state = self.rssm.observe(current_embed, prev_action, prev_state)\n",
        "            states_list.append(state)\n",
        "            prev_state = state\n",
        "\n",
        "        # 3. 画像の再構成 (Decoding)\n",
        "        # 全タイムステップの状態を結合\n",
        "        deter_seq = torch.stack([s['deter'] for s in states_list], dim=1)\n",
        "        stoch_seq = torch.stack([s['stoch'] for s in states_list], dim=1)\n",
        "\n",
        "        # デコーダーに一気に入力\n",
        "        flat_deter = deter_seq.view(b * t, -1)\n",
        "        flat_stoch = stoch_seq.view(b * t, -1)\n",
        "\n",
        "        recon_imgs = self.decoder({'deter': flat_deter, 'stoch': flat_stoch})\n",
        "\n",
        "        # 元の画像サイズに戻す (Batch, Seq, C, H, W)\n",
        "        # Encoder/Decoderの調整によりサイズが多少ズレる可能性があるため、ここで確認が必要\n",
        "        recon_imgs = recon_imgs.view(b, t, 3, 56, 56) # Minigridの拡大サイズ(56)に合わせる\n",
        "\n",
        "        return recon_imgs, states_list\n",
        "\n",
        "print(\"World Model (Nano-Dreamer) assembled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAW_fnEGHCAk",
        "outputId": "0ed70216-45cc-4081-94f1-8231a73c9eca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "World Model (Nano-Dreamer) assembled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 8: The \"Smoke Test\" (Sanity Check)\n",
        "本格的な学習を始める前に、ダミーデータ（乱数で作った偽の映像）を流し込んで、回路がショートしないか確認。\n",
        "\n",
        "このテストで、入力した画像と、出力された画像のサイズが一致するかを検証しました。"
      ],
      "metadata": {
        "id": "sShS-4ZJHc6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sanity Check Protocol\n",
        "def smoke_test():\n",
        "    print(\"Initiating Smoke Test...\")\n",
        "\n",
        "    # 1. Prepare Dummy Data\n",
        "    # Batch=2, Time=5, Channel=3, Height=56, Width=56\n",
        "    # Plan Bで確認した \"56x56\" の画像サイズを想定\n",
        "    dummy_obs = torch.randn(2, 5, 3, 56, 56)\n",
        "    dummy_actions = torch.randn(2, 5, 7) # Minigridの行動空間(7)\n",
        "\n",
        "    # 2. Initialize Model\n",
        "    # action_dim=7 (Unlock環境の行動数)\n",
        "    model = WorldModel(action_dim=7)\n",
        "\n",
        "    # 3. Forward Pass\n",
        "    try:\n",
        "        recon, states = model(dummy_obs, dummy_actions)\n",
        "\n",
        "        print(\"\\n=== Test Successful ===\")\n",
        "        print(f\"Input Shape:  {dummy_obs.shape}\")\n",
        "        print(f\"Output Shape: {recon.shape}\")\n",
        "\n",
        "        # サイズチェック\n",
        "        if dummy_obs.shape == recon.shape:\n",
        "            print(\"Status: GREEN (Shapes match perfectly)\")\n",
        "        else:\n",
        "            print(\"Status: YELLOW (Shapes differ - Resize needed)\")\n",
        "            print(\"Explanation: Decoder output size often mismatches input size due to convolution arithmetic.\")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(\"\\n=== Test Failed (Expected) ===\")\n",
        "        print(f\"Error Message: {e}\")\n",
        "        print(\"Diagnosis: The Decoder output size did not match the expected view size.\")\n",
        "\n",
        "smoke_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ohhCTcpHb9R",
        "outputId": "4ab37821-9264-4fa2-ebbd-6e7d0f610ad5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating Smoke Test...\n",
            "\n",
            "=== Test Failed (Expected) ===\n",
            "Error Message: shape '[2, 5, 3, 56, 56]' is invalid for input of size 115320\n",
            "Diagnosis: The Decoder output size did not match the expected view size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step8 のEroor Messageに対して,出力された画像を強制的に56x56にリサイズする処理を追加"
      ],
      "metadata": {
        "id": "QqCNfM2p9rzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixed Decoder with Auto-Resize\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim=64, deter_dim=200, stoch_dim=30):\n",
        "        super().__init__()\n",
        "        self.input_dim = deter_dim + stoch_dim\n",
        "\n",
        "        # ConvTransposeのパラメータは変更せず、そのまま使います\n",
        "        self.fc = nn.Linear(self.input_dim, 64 * 5 * 5)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.Unflatten(1, (64, 5, 5)),\n",
        "            nn.ConvTranspose2d(64, 32, 5, stride=2), # -> 13x13\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 5, stride=2), # -> 29x29\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, 6, stride=2),  # -> 62x62\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.cat([state['deter'], state['stoch']], dim=1)\n",
        "        x = self.fc(x)\n",
        "        raw_img = self.deconv(x)\n",
        "\n",
        "        # === THE FIX: Force Resize to 56x56 ===\n",
        "        # バイリニア補間を使って、目的のサイズに縮小/拡大します\n",
        "        return F.interpolate(raw_img, size=(56, 56), mode='bilinear', align_corners=False)\n",
        "\n",
        "#Re-Define WorldModel to use the new Decoder\n",
        "class WorldModel(nn.Module):\n",
        "    def __init__(self, action_dim, lr=1e-4):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.rssm = RSSM(action_dim)\n",
        "        self.decoder = Decoder()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, obs, actions):\n",
        "        b, t, c, h, w = obs.shape\n",
        "\n",
        "        # 1. Encode\n",
        "        embed = self.encoder(obs.view(b * t, c, h, w))\n",
        "        embed = embed.view(b, t, -1)\n",
        "\n",
        "        # 2. RSSM Loop\n",
        "        prev_state = self.rssm.initial_state(b)\n",
        "        states_list = []\n",
        "        for i in range(t):\n",
        "            current_embed = embed[:, i]\n",
        "            prev_action = actions[:, i]\n",
        "            state = self.rssm.observe(current_embed, prev_action, prev_state)\n",
        "            states_list.append(state)\n",
        "            prev_state = state\n",
        "\n",
        "        # 3. Decode\n",
        "        deter_seq = torch.stack([s['deter'] for s in states_list], dim=1)\n",
        "        stoch_seq = torch.stack([s['stoch'] for s in states_list], dim=1)\n",
        "\n",
        "        flat_deter = deter_seq.view(b * t, -1)\n",
        "        flat_stoch = stoch_seq.view(b * t, -1)\n",
        "\n",
        "        recon_imgs = self.decoder({'deter': flat_deter, 'stoch': flat_stoch})\n",
        "\n",
        "        recon_imgs = recon_imgs.view(b, t, 3, 56, 56)\n",
        "\n",
        "        return recon_imgs, states_list\n",
        "\n",
        "print(\"Decoder fixed with Auto-Resize.\")\n",
        "\n",
        "# --- Retry Smoke Test ---\n",
        "# 再度テストを実行\n",
        "smoke_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzZkmZnoIM4E",
        "outputId": "a7ae99af-d85d-476a-a6e3-7ef5be84df50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder fixed with Auto-Resize.\n",
            "Initiating Smoke Test...\n",
            "\n",
            "=== Test Successful ===\n",
            "Input Shape:  torch.Size([2, 5, 3, 56, 56])\n",
            "Output Shape: torch.Size([2, 5, 3, 56, 56])\n",
            "Status: GREEN (Shapes match perfectly)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step9: The \"First Breath\" (Training Loop)\n",
        "\n",
        "世界モデルに、Minigridの世界を見せて学習を行う。\n",
        "これまで作った全てのパーツ（環境、Encoder、Decoder）を統合し、AIが「世界を理解していく過程を確認"
      ],
      "metadata": {
        "id": "3Qq_mcej9tSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Integration: Training the World Model\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from minigrid.wrappers import FullyObsWrapper, ImgObsWrapper # Added ImgObsWrapper\n",
        "from gymnasium.wrappers import ResizeObservation # Corrected import for resizing wrapper\n",
        "\n",
        "# Define make_env function\n",
        "def make_env(env_name='MiniGrid-Unlock-v0', render_mode='rgb_array', with_cbet=False):\n",
        "    \"\"\"\n",
        "    Minigrid環境を作成し、DreamerV3に必要なラッパーを適用するヘルパー関数。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        env = gym.make(env_name, render_mode=render_mode)\n",
        "    except gym.error.Error:\n",
        "        print(f\"{env_name} not found, using MiniGrid-DoorKey-5x5-v0 instead.\")\n",
        "        env = gym.make('MiniGrid-DoorKey-5x5-v0', render_mode=render_mode)\n",
        "\n",
        "    # DreamerV3はエージェントのFOVではなく、マップ全体を観察するためFullyObsWrapperを適用\n",
        "    env = FullyObsWrapper(env)\n",
        "    # 画像のみを抽出してobservation_spaceをBoxにする\n",
        "    env = ImgObsWrapper(env)\n",
        "    # エンコーダの入力サイズに合わせるため、画像を56x56にリサイズ\n",
        "    env = ResizeObservation(env, (56, 56)) # Using ResizeObservation from gymnasium.wrappers\n",
        "\n",
        "    # CBETRewardは環境ラッパーとしてではなく、学習ループ内で別途計算される想定\n",
        "    # そのため、ここではwith_cbetの処理は環境に直接は適用しない\n",
        "    return env\n",
        "\n",
        "def train_step(model, obs_batch, action_batch):\n",
        "    \"\"\"\n",
        "    1ステップ分の学習を行う関数\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.optimizer.zero_grad()\n",
        "\n",
        "    # 1. Forward Pass (夢を見る)\n",
        "    recon, states = model(obs_batch, action_batch)     # recon: 再構成された画像, states: 脳内の状態\n",
        "\n",
        "    # 2. Compute Loss (現実とのズレを測る)\n",
        "    # A. Reconstruction Loss (見たままを想像できているか？)\n",
        "    # 画像の差分(MSE)を計算\n",
        "    rec_loss = F.mse_loss(recon, obs_batch)\n",
        "\n",
        "    # B. KL Divergence Loss (複雑すぎる夢を見ていないか？)\n",
        "    # 予測(Prior)と現実(Posterior)の乖離を抑える\n",
        "    # 簡易実装: PriorとPosteriorの平均・分散の差を計算\n",
        "    kl_loss = 0\n",
        "    for s in states:\n",
        "        # Prior(予測) vs Posterior(現実)\n",
        "        # 本来は正規分布同士のKLですが、ここでは簡易的に平均値の差分で見ます\n",
        "        prior_mean, post_mean = s['deter'], s['deter'] # 簡易化のため\n",
        "        # ※本来はRSSM内でPrior/Postを分けて出力する必要がありますが、\n",
        "        # 今回は \"rec_loss\" が減ることを主目的にします。\n",
        "\n",
        "    # Total Loss\n",
        "    total_loss = rec_loss # + 0.1 * kl_loss (今回は再構成のみに集中)\n",
        "\n",
        "    # 3. Backward Pass (脳を修正する)\n",
        "    total_loss.backward()\n",
        "\n",
        "    # 勾配爆発を防ぐ (Cliff Walking)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    model.optimizer.step()\n",
        "\n",
        "    return total_loss.item()\n",
        "\n",
        "# --- The Experiment Loop ---\n",
        "print(\"Collecting experience from Minigrid...\")\n",
        "\n",
        "# 1. データを集める (Plan Bの環境を使用)\n",
        "# 環境を再作成 (56x56サイズ)\n",
        "env = make_env(with_cbet=True) # The 'with_cbet' argument is passed but not used in `make_env` for now.\n",
        "obs, info = env.reset() # env.reset() returns (observation, info) since gymnasium 0.26.0\n",
        "\n",
        "data_obs = []\n",
        "data_actions = []\n",
        "\n",
        "# 100ステップ分、適当に動いてデータを集める\n",
        "for _ in range(100):\n",
        "    # ランダムに行動 (0~6)\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # データを記録 (Batch=1, Channel=3, 56, 56)\n",
        "    # PPOのWrapper経由なので、obsは既に (56, 56, 3) などの形\n",
        "    # PyTorch用に (3, 56, 56) に変換が必要\n",
        "    # ImgObsWrapperを適用したため、obsは直接画像データになります\n",
        "    obs_img = obs # Access the 'image' key from the observation dict\n",
        "    obs_tensor = torch.tensor(obs_img.copy()).permute(2, 0, 1).float() / 255.0\n",
        "    action_onehot = F.one_hot(torch.tensor(action), num_classes=7).float()\n",
        "\n",
        "    data_obs.append(obs_tensor)\n",
        "    data_actions.append(action_onehot)\n",
        "\n",
        "    # env.step() returns (observation, reward, terminated, truncated, info) in gymnasium.\n",
        "    # The current code only expects (obs, reward, done, info).\n",
        "    # Need to update to handle `terminated` and `truncated`.\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    if done:\n",
        "        obs, info = env.reset() # Reset returns (observation, info)\n",
        "\n",
        "# バッチ化 (Batch=1, Time=100, ...)\n",
        "batch_obs = torch.stack(data_obs).unsqueeze(0) # -> (1, 100, 3, 56, 56)\n",
        "batch_actions = torch.stack(data_actions).unsqueeze(0) # -> (1, 100, 7)\n",
        "\n",
        "print(f\"Dataset collected. Shape: {batch_obs.shape}\")\n",
        "print(\"Starting training (The 'First Breath')...\")\n",
        "\n",
        "# 2. 学習ループ\n",
        "model = WorldModel(action_dim=7)\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss = train_step(model, batch_obs, batch_actions)\n",
        "    print(f\"Epoch {epoch+1}: Reconstruction Loss = {loss:.6f}\")\n",
        "\n",
        "print(\"\\n=== MISSION COMPLETE ===\")\n",
        "print(\"The World Model is learning to reconstruct reality.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXm_qhG9I0_C",
        "outputId": "188a4aa5-761a-4316-fe6c-77aa9913fb4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting experience from Minigrid...\n",
            "Dataset collected. Shape: torch.Size([1, 100, 3, 56, 56])\n",
            "Starting training (The 'First Breath')...\n",
            "Epoch 1: Reconstruction Loss = 0.001244\n",
            "Epoch 2: Reconstruction Loss = 0.001216\n",
            "Epoch 3: Reconstruction Loss = 0.001192\n",
            "Epoch 4: Reconstruction Loss = 0.001167\n",
            "Epoch 5: Reconstruction Loss = 0.001142\n",
            "Epoch 6: Reconstruction Loss = 0.001118\n",
            "Epoch 7: Reconstruction Loss = 0.001093\n",
            "Epoch 8: Reconstruction Loss = 0.001070\n",
            "Epoch 9: Reconstruction Loss = 0.001047\n",
            "Epoch 10: Reconstruction Loss = 0.001023\n",
            "\n",
            "=== MISSION COMPLETE ===\n",
            "The World Model is learning to reconstruct reality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step10: ライブラリの再インストール"
      ],
      "metadata": {
        "id": "vPPEToMX9vpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#環境のセットアップ\n",
        "!pip install stable-baselines3 shimmy>=0.2.1 minigrid"
      ],
      "metadata": {
        "id": "OCS_cgUFSs3b"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step11:The \"All-In-One\" Execution (全回路の再接続)\n",
        "これまで作ってきた全てのパーツ（RSSM, Encoder, Decoder, WorldModel, CBET, Training Loop）を一つのセルに統合。"
      ],
      "metadata": {
        "id": "efqR8cm39wDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "# SECTION 1: ENVIRONMENT SETUP (Inherited from Plan B)\n",
        "\n",
        "class CBETReward:\n",
        "    \"\"\"好奇心報酬を計算するクラス (Plan Bの成果物)\"\"\"\n",
        "    def __init__(self):\n",
        "        self.state_counts = {}\n",
        "        self.change_counts = {}\n",
        "\n",
        "    def get_hash(self, array):\n",
        "        return hashlib.md5(array.tobytes()).hexdigest()\n",
        "\n",
        "    def compute(self, obs, next_obs):\n",
        "        s_hash = self.get_hash(obs)\n",
        "        change = next_obs.astype(np.float32) - obs.astype(np.float32)\n",
        "        c_hash = self.get_hash(change)\n",
        "\n",
        "        self.state_counts[s_hash] = self.state_counts.get(s_hash, 0) + 1\n",
        "        self.change_counts[c_hash] = self.change_counts.get(c_hash, 0) + 1\n",
        "\n",
        "        # ゼロ除算防止\n",
        "        denom = np.sqrt(self.state_counts[s_hash]) + np.sqrt(self.change_counts[c_hash])\n",
        "        return 1.0 / (denom + 1e-5)\n",
        "\n",
        "class CBETEnvWrapper(gym.Wrapper):\n",
        "    \"\"\"環境に好奇心報酬を追加するラッパー\"\"\"\n",
        "    def __init__(self, env, alpha=1.0):\n",
        "        super().__init__(env)\n",
        "        self.cbet = CBETReward()\n",
        "        self.last_obs = None\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.last_obs = obs\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        intrinsic_reward = 0.0\n",
        "        if self.last_obs is not None:\n",
        "            intrinsic_reward = self.cbet.compute(self.last_obs, next_obs)\n",
        "        self.last_obs = next_obs\n",
        "        total_reward = reward + (self.alpha * intrinsic_reward)\n",
        "        return next_obs, total_reward, terminated, truncated, info\n",
        "\n",
        "# 【重要】ここが NameError の原因だった関数です。ここで定義します。\n",
        "def make_env(with_cbet=True):\n",
        "    env = gym.make('MiniGrid-Unlock-v0', render_mode='rgb_array')\n",
        "    env = RGBImgPartialObsWrapper(env) # 56x56に拡大\n",
        "    env = ImgObsWrapper(env)\n",
        "    if with_cbet:\n",
        "        env = CBETEnvWrapper(env, alpha=1.0)\n",
        "    return Monitor(env)\n",
        "\n",
        "\n",
        "# SECTION 2: WORLD MODEL ARCHITECTURE (The Core of Plan A)\n",
        "# ここが今回の主役です。Plan Bには存在しなかったEncoderを作成。\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.fc = nn.Linear(64 * 5 * 5, embed_dim)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = self.conv(obs)\n",
        "        return self.fc(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim=64, deter_dim=200, stoch_dim=30):\n",
        "        super().__init__()\n",
        "        self.input_dim = deter_dim + stoch_dim\n",
        "        self.fc = nn.Linear(self.input_dim, 64 * 5 * 5)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.Unflatten(1, (64, 5, 5)),\n",
        "            nn.ConvTranspose2d(64, 32, 5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, 6, stride=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.cat([state['deter'], state['stoch']], dim=1)\n",
        "        x = self.fc(x)\n",
        "        raw_img = self.deconv(x)\n",
        "        # 62x62 -> 56x56 強制リサイズ (以前のエラー対策)\n",
        "        return F.interpolate(raw_img, size=(56, 56), mode='bilinear', align_corners=False)\n",
        "\n",
        "class RSSM(nn.Module):\n",
        "    \"\"\"リカレント状態空間モデル (Plan Aの脳)\"\"\"\n",
        "    def __init__(self, action_dim, embed_dim=64, deter_dim=200, stoch_dim=30):\n",
        "        super().__init__()\n",
        "        self.deter_dim = deter_dim\n",
        "        self.stoch_dim = stoch_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n",
        "        self.fc_prior = nn.Linear(deter_dim, stoch_dim * 2)\n",
        "        self.fc_post = nn.Linear(deter_dim + embed_dim, stoch_dim * 2)\n",
        "\n",
        "    def initial_state(self, batch_size):\n",
        "        return dict(\n",
        "            mean  = torch.zeros(batch_size, self.stoch_dim),\n",
        "            std   = torch.zeros(batch_size, self.stoch_dim),\n",
        "            stoch = torch.zeros(batch_size, self.stoch_dim),\n",
        "            deter = torch.zeros(batch_size, self.deter_dim)\n",
        "        )\n",
        "\n",
        "    def observe(self, embed, prev_action, prev_state):\n",
        "        gru_input = torch.cat([prev_state['stoch'], prev_action], dim=1)\n",
        "        deter = self.gru(gru_input, prev_state['deter'])\n",
        "        x = torch.cat([deter, embed], dim=1)\n",
        "        stats = self.fc_post(x)\n",
        "        mean, std = torch.chunk(stats, 2, dim=1)\n",
        "        std = F.softplus(std) + 0.1\n",
        "        noise = torch.randn_like(mean)\n",
        "        stoch = mean + std * noise\n",
        "        return {'mean': mean, 'std': std, 'stoch': stoch, 'deter': deter}\n",
        "\n",
        "class WorldModel(nn.Module):\n",
        "    \"\"\"Plan Aの完成形モデル\"\"\"\n",
        "    def __init__(self, action_dim, lr=1e-4):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.rssm = RSSM(action_dim)\n",
        "        self.decoder = Decoder()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, obs, actions):\n",
        "        b, t, c, h, w = obs.shape\n",
        "        embed = self.encoder(obs.view(b * t, c, h, w))\n",
        "        embed = embed.view(b, t, -1)\n",
        "        prev_state = self.rssm.initial_state(b)\n",
        "        states_list = []\n",
        "        for i in range(t):\n",
        "            current_embed = embed[:, i]\n",
        "            prev_action = actions[:, i]\n",
        "            state = self.rssm.observe(current_embed, prev_action, prev_state)\n",
        "            states_list.append(state)\n",
        "            prev_state = state\n",
        "\n",
        "        deter_seq = torch.stack([s['deter'] for s in states_list], dim=1)\n",
        "        stoch_seq = torch.stack([s['stoch'] for s in states_list], dim=1)\n",
        "        flat_deter = deter_seq.view(b * t, -1)\n",
        "        flat_stoch = stoch_seq.view(b * t, -1)\n",
        "        recon_imgs = self.decoder({'deter': flat_deter, 'stoch': flat_stoch})\n",
        "        recon_imgs = recon_imgs.view(b, t, 3, 56, 56)\n",
        "        return recon_imgs, states_list\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 3: TRAINING LOOP (The \"First Breath\" of Plan A)\n",
        "# ==============================================================================\n",
        "\n",
        "def train_step(model, obs_batch, action_batch):\n",
        "    model.train()\n",
        "    model.optimizer.zero_grad()\n",
        "    # 夢を見る (Forward)\n",
        "    recon, states = model(obs_batch, action_batch)\n",
        "    # 現実との誤差を計算 (Reconstruction Loss)\n",
        "    rec_loss = F.mse_loss(recon, obs_batch)\n",
        "    # 脳を修正 (Backward)\n",
        "    rec_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    model.optimizer.step()\n",
        "    return rec_loss.item()\n",
        "\n",
        "print(\"1. Collecting experience from Minigrid (using make_env)...\")\n",
        "# ここで make_env を使うため、このセル内で定義されている必要があります\n",
        "env = make_env(with_cbet=True)\n",
        "obs, _ = env.reset()\n",
        "data_obs = []\n",
        "data_actions = []\n",
        "\n",
        "# データ収集ループ\n",
        "for _ in range(100):\n",
        "    action = env.action_space.sample()\n",
        "    obs_tensor = torch.tensor(obs.copy()).permute(2, 0, 1).float() / 255.0\n",
        "    action_onehot = F.one_hot(torch.tensor(action), num_classes=7).float()\n",
        "    data_obs.append(obs_tensor)\n",
        "    data_actions.append(action_onehot)\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    if terminated or truncated:\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "batch_obs = torch.stack(data_obs).unsqueeze(0)\n",
        "batch_actions = torch.stack(data_actions).unsqueeze(0)\n",
        "\n",
        "print(f\"Dataset collected. Shape: {batch_obs.shape}\")\n",
        "print(\"2. Starting World Model training (Plan A)...\")\n",
        "\n",
        "model = WorldModel(action_dim=7)\n",
        "\n",
        "# ロス値を保存するためのリスト\n",
        "loss_history = []\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(1000):\n",
        "    loss = train_step(model, batch_obs, batch_actions)\n",
        "    loss_history.append(loss)\n",
        "    print(f\"Epoch {epoch+1}: Reconstruction Loss = {loss:.6f}\")\n",
        "\n",
        "print(\"\\n=== PLAN A: MISSION COMPLETE ===\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DnK_2sbQ1_1",
        "outputId": "a3b616f0-f55d-4b05-e93e-cd72edd1efce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Collecting experience from Minigrid (using make_env)...\n",
            "Dataset collected. Shape: torch.Size([1, 100, 3, 56, 56])\n",
            "2. Starting World Model training (Plan A)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Reconstruction Loss = 0.084771\n",
            "Epoch 2: Reconstruction Loss = 0.084578\n",
            "Epoch 3: Reconstruction Loss = 0.084396\n",
            "Epoch 4: Reconstruction Loss = 0.084215\n",
            "Epoch 5: Reconstruction Loss = 0.084029\n",
            "Epoch 6: Reconstruction Loss = 0.083840\n",
            "Epoch 7: Reconstruction Loss = 0.083667\n",
            "Epoch 8: Reconstruction Loss = 0.083485\n",
            "Epoch 9: Reconstruction Loss = 0.083310\n",
            "Epoch 10: Reconstruction Loss = 0.083134\n",
            "Epoch 11: Reconstruction Loss = 0.082953\n",
            "Epoch 12: Reconstruction Loss = 0.082774\n",
            "Epoch 13: Reconstruction Loss = 0.082594\n",
            "Epoch 14: Reconstruction Loss = 0.082421\n",
            "Epoch 15: Reconstruction Loss = 0.082228\n",
            "Epoch 16: Reconstruction Loss = 0.082044\n",
            "Epoch 17: Reconstruction Loss = 0.081849\n",
            "Epoch 18: Reconstruction Loss = 0.081693\n",
            "Epoch 19: Reconstruction Loss = 0.081487\n",
            "Epoch 20: Reconstruction Loss = 0.081324\n",
            "Epoch 21: Reconstruction Loss = 0.081111\n",
            "Epoch 22: Reconstruction Loss = 0.080901\n",
            "Epoch 23: Reconstruction Loss = 0.080657\n",
            "Epoch 24: Reconstruction Loss = 0.080396\n",
            "Epoch 25: Reconstruction Loss = 0.080183\n",
            "Epoch 26: Reconstruction Loss = 0.079935\n",
            "Epoch 27: Reconstruction Loss = 0.079631\n",
            "Epoch 28: Reconstruction Loss = 0.079363\n",
            "Epoch 29: Reconstruction Loss = 0.078939\n",
            "Epoch 30: Reconstruction Loss = 0.078612\n",
            "Epoch 31: Reconstruction Loss = 0.078055\n",
            "Epoch 32: Reconstruction Loss = 0.077568\n",
            "Epoch 33: Reconstruction Loss = 0.076885\n",
            "Epoch 34: Reconstruction Loss = 0.076137\n",
            "Epoch 35: Reconstruction Loss = 0.075161\n",
            "Epoch 36: Reconstruction Loss = 0.073797\n",
            "Epoch 37: Reconstruction Loss = 0.072445\n",
            "Epoch 38: Reconstruction Loss = 0.071038\n",
            "Epoch 39: Reconstruction Loss = 0.069045\n",
            "Epoch 40: Reconstruction Loss = 0.066619\n",
            "Epoch 41: Reconstruction Loss = 0.064577\n",
            "Epoch 42: Reconstruction Loss = 0.061929\n",
            "Epoch 43: Reconstruction Loss = 0.058888\n",
            "Epoch 44: Reconstruction Loss = 0.057046\n",
            "Epoch 45: Reconstruction Loss = 0.055953\n",
            "Epoch 46: Reconstruction Loss = 0.055698\n",
            "Epoch 47: Reconstruction Loss = 0.056026\n",
            "Epoch 48: Reconstruction Loss = 0.055777\n",
            "Epoch 49: Reconstruction Loss = 0.054574\n",
            "Epoch 50: Reconstruction Loss = 0.052702\n",
            "Epoch 51: Reconstruction Loss = 0.050960\n",
            "Epoch 52: Reconstruction Loss = 0.049524\n",
            "Epoch 53: Reconstruction Loss = 0.048245\n",
            "Epoch 54: Reconstruction Loss = 0.047300\n",
            "Epoch 55: Reconstruction Loss = 0.046702\n",
            "Epoch 56: Reconstruction Loss = 0.046002\n",
            "Epoch 57: Reconstruction Loss = 0.045678\n",
            "Epoch 58: Reconstruction Loss = 0.044915\n",
            "Epoch 59: Reconstruction Loss = 0.044410\n",
            "Epoch 60: Reconstruction Loss = 0.043496\n",
            "Epoch 61: Reconstruction Loss = 0.043041\n",
            "Epoch 62: Reconstruction Loss = 0.042657\n",
            "Epoch 63: Reconstruction Loss = 0.042011\n",
            "Epoch 64: Reconstruction Loss = 0.041491\n",
            "Epoch 65: Reconstruction Loss = 0.041476\n",
            "Epoch 66: Reconstruction Loss = 0.041523\n",
            "Epoch 67: Reconstruction Loss = 0.040908\n",
            "Epoch 68: Reconstruction Loss = 0.040571\n",
            "Epoch 69: Reconstruction Loss = 0.040543\n",
            "Epoch 70: Reconstruction Loss = 0.039974\n",
            "Epoch 71: Reconstruction Loss = 0.039864\n",
            "Epoch 72: Reconstruction Loss = 0.039598\n",
            "Epoch 73: Reconstruction Loss = 0.039184\n",
            "Epoch 74: Reconstruction Loss = 0.039168\n",
            "Epoch 75: Reconstruction Loss = 0.038702\n",
            "Epoch 76: Reconstruction Loss = 0.038513\n",
            "Epoch 77: Reconstruction Loss = 0.038327\n",
            "Epoch 78: Reconstruction Loss = 0.038020\n",
            "Epoch 79: Reconstruction Loss = 0.037817\n",
            "Epoch 80: Reconstruction Loss = 0.037786\n",
            "Epoch 81: Reconstruction Loss = 0.037469\n",
            "Epoch 82: Reconstruction Loss = 0.037332\n",
            "Epoch 83: Reconstruction Loss = 0.037092\n",
            "Epoch 84: Reconstruction Loss = 0.036945\n",
            "Epoch 85: Reconstruction Loss = 0.037015\n",
            "Epoch 86: Reconstruction Loss = 0.036683\n",
            "Epoch 87: Reconstruction Loss = 0.036628\n",
            "Epoch 88: Reconstruction Loss = 0.036616\n",
            "Epoch 89: Reconstruction Loss = 0.036098\n",
            "Epoch 90: Reconstruction Loss = 0.036155\n",
            "Epoch 91: Reconstruction Loss = 0.035940\n",
            "Epoch 92: Reconstruction Loss = 0.035939\n",
            "Epoch 93: Reconstruction Loss = 0.035854\n",
            "Epoch 94: Reconstruction Loss = 0.035539\n",
            "Epoch 95: Reconstruction Loss = 0.035407\n",
            "Epoch 96: Reconstruction Loss = 0.035413\n",
            "Epoch 97: Reconstruction Loss = 0.035395\n",
            "Epoch 98: Reconstruction Loss = 0.035359\n",
            "Epoch 99: Reconstruction Loss = 0.035009\n",
            "Epoch 100: Reconstruction Loss = 0.034872\n",
            "Epoch 101: Reconstruction Loss = 0.034736\n",
            "Epoch 102: Reconstruction Loss = 0.034671\n",
            "Epoch 103: Reconstruction Loss = 0.034666\n",
            "Epoch 104: Reconstruction Loss = 0.034603\n",
            "Epoch 105: Reconstruction Loss = 0.034404\n",
            "Epoch 106: Reconstruction Loss = 0.034255\n",
            "Epoch 107: Reconstruction Loss = 0.034305\n",
            "Epoch 108: Reconstruction Loss = 0.034074\n",
            "Epoch 109: Reconstruction Loss = 0.033954\n",
            "Epoch 110: Reconstruction Loss = 0.034043\n",
            "Epoch 111: Reconstruction Loss = 0.033789\n",
            "Epoch 112: Reconstruction Loss = 0.033720\n",
            "Epoch 113: Reconstruction Loss = 0.033437\n",
            "Epoch 114: Reconstruction Loss = 0.033382\n",
            "Epoch 115: Reconstruction Loss = 0.033346\n",
            "Epoch 116: Reconstruction Loss = 0.033167\n",
            "Epoch 117: Reconstruction Loss = 0.033143\n",
            "Epoch 118: Reconstruction Loss = 0.033025\n",
            "Epoch 119: Reconstruction Loss = 0.032959\n",
            "Epoch 120: Reconstruction Loss = 0.032817\n",
            "Epoch 121: Reconstruction Loss = 0.032764\n",
            "Epoch 122: Reconstruction Loss = 0.032937\n",
            "Epoch 123: Reconstruction Loss = 0.032562\n",
            "Epoch 124: Reconstruction Loss = 0.032455\n",
            "Epoch 125: Reconstruction Loss = 0.032243\n",
            "Epoch 126: Reconstruction Loss = 0.032273\n",
            "Epoch 127: Reconstruction Loss = 0.032051\n",
            "Epoch 128: Reconstruction Loss = 0.032075\n",
            "Epoch 129: Reconstruction Loss = 0.031901\n",
            "Epoch 130: Reconstruction Loss = 0.031823\n",
            "Epoch 131: Reconstruction Loss = 0.031560\n",
            "Epoch 132: Reconstruction Loss = 0.031643\n",
            "Epoch 133: Reconstruction Loss = 0.031509\n",
            "Epoch 134: Reconstruction Loss = 0.031419\n",
            "Epoch 135: Reconstruction Loss = 0.031274\n",
            "Epoch 136: Reconstruction Loss = 0.031167\n",
            "Epoch 137: Reconstruction Loss = 0.031011\n",
            "Epoch 138: Reconstruction Loss = 0.030913\n",
            "Epoch 139: Reconstruction Loss = 0.030815\n",
            "Epoch 140: Reconstruction Loss = 0.030524\n",
            "Epoch 141: Reconstruction Loss = 0.030494\n",
            "Epoch 142: Reconstruction Loss = 0.030252\n",
            "Epoch 143: Reconstruction Loss = 0.030283\n",
            "Epoch 144: Reconstruction Loss = 0.030113\n",
            "Epoch 145: Reconstruction Loss = 0.029955\n",
            "Epoch 146: Reconstruction Loss = 0.029836\n",
            "Epoch 147: Reconstruction Loss = 0.029780\n",
            "Epoch 148: Reconstruction Loss = 0.029629\n",
            "Epoch 149: Reconstruction Loss = 0.029470\n",
            "Epoch 150: Reconstruction Loss = 0.029327\n",
            "Epoch 151: Reconstruction Loss = 0.029241\n",
            "Epoch 152: Reconstruction Loss = 0.029117\n",
            "Epoch 153: Reconstruction Loss = 0.028997\n",
            "Epoch 154: Reconstruction Loss = 0.028875\n",
            "Epoch 155: Reconstruction Loss = 0.028755\n",
            "Epoch 156: Reconstruction Loss = 0.028595\n",
            "Epoch 157: Reconstruction Loss = 0.028563\n",
            "Epoch 158: Reconstruction Loss = 0.028429\n",
            "Epoch 159: Reconstruction Loss = 0.028262\n",
            "Epoch 160: Reconstruction Loss = 0.028249\n",
            "Epoch 161: Reconstruction Loss = 0.028142\n",
            "Epoch 162: Reconstruction Loss = 0.027994\n",
            "Epoch 163: Reconstruction Loss = 0.027854\n",
            "Epoch 164: Reconstruction Loss = 0.027831\n",
            "Epoch 165: Reconstruction Loss = 0.027692\n",
            "Epoch 166: Reconstruction Loss = 0.027658\n",
            "Epoch 167: Reconstruction Loss = 0.027598\n",
            "Epoch 168: Reconstruction Loss = 0.027520\n",
            "Epoch 169: Reconstruction Loss = 0.027400\n",
            "Epoch 170: Reconstruction Loss = 0.027431\n",
            "Epoch 171: Reconstruction Loss = 0.027365\n",
            "Epoch 172: Reconstruction Loss = 0.027269\n",
            "Epoch 173: Reconstruction Loss = 0.027194\n",
            "Epoch 174: Reconstruction Loss = 0.027101\n",
            "Epoch 175: Reconstruction Loss = 0.027048\n",
            "Epoch 176: Reconstruction Loss = 0.027038\n",
            "Epoch 177: Reconstruction Loss = 0.026972\n",
            "Epoch 178: Reconstruction Loss = 0.026915\n",
            "Epoch 179: Reconstruction Loss = 0.026862\n",
            "Epoch 180: Reconstruction Loss = 0.026835\n",
            "Epoch 181: Reconstruction Loss = 0.026747\n",
            "Epoch 182: Reconstruction Loss = 0.026706\n",
            "Epoch 183: Reconstruction Loss = 0.026686\n",
            "Epoch 184: Reconstruction Loss = 0.026611\n",
            "Epoch 185: Reconstruction Loss = 0.026528\n",
            "Epoch 186: Reconstruction Loss = 0.026486\n",
            "Epoch 187: Reconstruction Loss = 0.026414\n",
            "Epoch 188: Reconstruction Loss = 0.026404\n",
            "Epoch 189: Reconstruction Loss = 0.026348\n",
            "Epoch 190: Reconstruction Loss = 0.026259\n",
            "Epoch 191: Reconstruction Loss = 0.026220\n",
            "Epoch 192: Reconstruction Loss = 0.026191\n",
            "Epoch 193: Reconstruction Loss = 0.026102\n",
            "Epoch 194: Reconstruction Loss = 0.026051\n",
            "Epoch 195: Reconstruction Loss = 0.026002\n",
            "Epoch 196: Reconstruction Loss = 0.025924\n",
            "Epoch 197: Reconstruction Loss = 0.025899\n",
            "Epoch 198: Reconstruction Loss = 0.025820\n",
            "Epoch 199: Reconstruction Loss = 0.025743\n",
            "Epoch 200: Reconstruction Loss = 0.025691\n",
            "Epoch 201: Reconstruction Loss = 0.025603\n",
            "Epoch 202: Reconstruction Loss = 0.025561\n",
            "Epoch 203: Reconstruction Loss = 0.025495\n",
            "Epoch 204: Reconstruction Loss = 0.025435\n",
            "Epoch 205: Reconstruction Loss = 0.025373\n",
            "Epoch 206: Reconstruction Loss = 0.025312\n",
            "Epoch 207: Reconstruction Loss = 0.025186\n",
            "Epoch 208: Reconstruction Loss = 0.025170\n",
            "Epoch 209: Reconstruction Loss = 0.025115\n",
            "Epoch 210: Reconstruction Loss = 0.024990\n",
            "Epoch 211: Reconstruction Loss = 0.024910\n",
            "Epoch 212: Reconstruction Loss = 0.024832\n",
            "Epoch 213: Reconstruction Loss = 0.024744\n",
            "Epoch 214: Reconstruction Loss = 0.024651\n",
            "Epoch 215: Reconstruction Loss = 0.024544\n",
            "Epoch 216: Reconstruction Loss = 0.024476\n",
            "Epoch 217: Reconstruction Loss = 0.024366\n",
            "Epoch 218: Reconstruction Loss = 0.024269\n",
            "Epoch 219: Reconstruction Loss = 0.024185\n",
            "Epoch 220: Reconstruction Loss = 0.024116\n",
            "Epoch 221: Reconstruction Loss = 0.024018\n",
            "Epoch 222: Reconstruction Loss = 0.023845\n",
            "Epoch 223: Reconstruction Loss = 0.023772\n",
            "Epoch 224: Reconstruction Loss = 0.023654\n",
            "Epoch 225: Reconstruction Loss = 0.023550\n",
            "Epoch 226: Reconstruction Loss = 0.023389\n",
            "Epoch 227: Reconstruction Loss = 0.023327\n",
            "Epoch 228: Reconstruction Loss = 0.023174\n",
            "Epoch 229: Reconstruction Loss = 0.023076\n",
            "Epoch 230: Reconstruction Loss = 0.022940\n",
            "Epoch 231: Reconstruction Loss = 0.022824\n",
            "Epoch 232: Reconstruction Loss = 0.022720\n",
            "Epoch 233: Reconstruction Loss = 0.022562\n",
            "Epoch 234: Reconstruction Loss = 0.022472\n",
            "Epoch 235: Reconstruction Loss = 0.022351\n",
            "Epoch 236: Reconstruction Loss = 0.022217\n",
            "Epoch 237: Reconstruction Loss = 0.022093\n",
            "Epoch 238: Reconstruction Loss = 0.021955\n",
            "Epoch 239: Reconstruction Loss = 0.021860\n",
            "Epoch 240: Reconstruction Loss = 0.021747\n",
            "Epoch 241: Reconstruction Loss = 0.021578\n",
            "Epoch 242: Reconstruction Loss = 0.021488\n",
            "Epoch 243: Reconstruction Loss = 0.021373\n",
            "Epoch 244: Reconstruction Loss = 0.021262\n",
            "Epoch 245: Reconstruction Loss = 0.021146\n",
            "Epoch 246: Reconstruction Loss = 0.021004\n",
            "Epoch 247: Reconstruction Loss = 0.020899\n",
            "Epoch 248: Reconstruction Loss = 0.020789\n",
            "Epoch 249: Reconstruction Loss = 0.020678\n",
            "Epoch 250: Reconstruction Loss = 0.020545\n",
            "Epoch 251: Reconstruction Loss = 0.020439\n",
            "Epoch 252: Reconstruction Loss = 0.020319\n",
            "Epoch 253: Reconstruction Loss = 0.020210\n",
            "Epoch 254: Reconstruction Loss = 0.020129\n",
            "Epoch 255: Reconstruction Loss = 0.019991\n",
            "Epoch 256: Reconstruction Loss = 0.019872\n",
            "Epoch 257: Reconstruction Loss = 0.019763\n",
            "Epoch 258: Reconstruction Loss = 0.019658\n",
            "Epoch 259: Reconstruction Loss = 0.019561\n",
            "Epoch 260: Reconstruction Loss = 0.019422\n",
            "Epoch 261: Reconstruction Loss = 0.019318\n",
            "Epoch 262: Reconstruction Loss = 0.019197\n",
            "Epoch 263: Reconstruction Loss = 0.019088\n",
            "Epoch 264: Reconstruction Loss = 0.018965\n",
            "Epoch 265: Reconstruction Loss = 0.018874\n",
            "Epoch 266: Reconstruction Loss = 0.018762\n",
            "Epoch 267: Reconstruction Loss = 0.018665\n",
            "Epoch 268: Reconstruction Loss = 0.018549\n",
            "Epoch 269: Reconstruction Loss = 0.018429\n",
            "Epoch 270: Reconstruction Loss = 0.018332\n",
            "Epoch 271: Reconstruction Loss = 0.018216\n",
            "Epoch 272: Reconstruction Loss = 0.018108\n",
            "Epoch 273: Reconstruction Loss = 0.018012\n",
            "Epoch 274: Reconstruction Loss = 0.017877\n",
            "Epoch 275: Reconstruction Loss = 0.017780\n",
            "Epoch 276: Reconstruction Loss = 0.017667\n",
            "Epoch 277: Reconstruction Loss = 0.017574\n",
            "Epoch 278: Reconstruction Loss = 0.017449\n",
            "Epoch 279: Reconstruction Loss = 0.017340\n",
            "Epoch 280: Reconstruction Loss = 0.017246\n",
            "Epoch 281: Reconstruction Loss = 0.017119\n",
            "Epoch 282: Reconstruction Loss = 0.016994\n",
            "Epoch 283: Reconstruction Loss = 0.016875\n",
            "Epoch 284: Reconstruction Loss = 0.016779\n",
            "Epoch 285: Reconstruction Loss = 0.016630\n",
            "Epoch 286: Reconstruction Loss = 0.016497\n",
            "Epoch 287: Reconstruction Loss = 0.016385\n",
            "Epoch 288: Reconstruction Loss = 0.016261\n",
            "Epoch 289: Reconstruction Loss = 0.016128\n",
            "Epoch 290: Reconstruction Loss = 0.015996\n",
            "Epoch 291: Reconstruction Loss = 0.015868\n",
            "Epoch 292: Reconstruction Loss = 0.015721\n",
            "Epoch 293: Reconstruction Loss = 0.015596\n",
            "Epoch 294: Reconstruction Loss = 0.015466\n",
            "Epoch 295: Reconstruction Loss = 0.015326\n",
            "Epoch 296: Reconstruction Loss = 0.015182\n",
            "Epoch 297: Reconstruction Loss = 0.015049\n",
            "Epoch 298: Reconstruction Loss = 0.014918\n",
            "Epoch 299: Reconstruction Loss = 0.014766\n",
            "Epoch 300: Reconstruction Loss = 0.014636\n",
            "Epoch 301: Reconstruction Loss = 0.014494\n",
            "Epoch 302: Reconstruction Loss = 0.014360\n",
            "Epoch 303: Reconstruction Loss = 0.014223\n",
            "Epoch 304: Reconstruction Loss = 0.014089\n",
            "Epoch 305: Reconstruction Loss = 0.013960\n",
            "Epoch 306: Reconstruction Loss = 0.013831\n",
            "Epoch 307: Reconstruction Loss = 0.013684\n",
            "Epoch 308: Reconstruction Loss = 0.013552\n",
            "Epoch 309: Reconstruction Loss = 0.013406\n",
            "Epoch 310: Reconstruction Loss = 0.013268\n",
            "Epoch 311: Reconstruction Loss = 0.013145\n",
            "Epoch 312: Reconstruction Loss = 0.012999\n",
            "Epoch 313: Reconstruction Loss = 0.012856\n",
            "Epoch 314: Reconstruction Loss = 0.012720\n",
            "Epoch 315: Reconstruction Loss = 0.012590\n",
            "Epoch 316: Reconstruction Loss = 0.012447\n",
            "Epoch 317: Reconstruction Loss = 0.012314\n",
            "Epoch 318: Reconstruction Loss = 0.012181\n",
            "Epoch 319: Reconstruction Loss = 0.012068\n",
            "Epoch 320: Reconstruction Loss = 0.011945\n",
            "Epoch 321: Reconstruction Loss = 0.011830\n",
            "Epoch 322: Reconstruction Loss = 0.011719\n",
            "Epoch 323: Reconstruction Loss = 0.011611\n",
            "Epoch 324: Reconstruction Loss = 0.011509\n",
            "Epoch 325: Reconstruction Loss = 0.011405\n",
            "Epoch 326: Reconstruction Loss = 0.011306\n",
            "Epoch 327: Reconstruction Loss = 0.011214\n",
            "Epoch 328: Reconstruction Loss = 0.011111\n",
            "Epoch 329: Reconstruction Loss = 0.011017\n",
            "Epoch 330: Reconstruction Loss = 0.010917\n",
            "Epoch 331: Reconstruction Loss = 0.010823\n",
            "Epoch 332: Reconstruction Loss = 0.010722\n",
            "Epoch 333: Reconstruction Loss = 0.010646\n",
            "Epoch 334: Reconstruction Loss = 0.010544\n",
            "Epoch 335: Reconstruction Loss = 0.010458\n",
            "Epoch 336: Reconstruction Loss = 0.010359\n",
            "Epoch 337: Reconstruction Loss = 0.010265\n",
            "Epoch 338: Reconstruction Loss = 0.010176\n",
            "Epoch 339: Reconstruction Loss = 0.010085\n",
            "Epoch 340: Reconstruction Loss = 0.009983\n",
            "Epoch 341: Reconstruction Loss = 0.009904\n",
            "Epoch 342: Reconstruction Loss = 0.009806\n",
            "Epoch 343: Reconstruction Loss = 0.009722\n",
            "Epoch 344: Reconstruction Loss = 0.009647\n",
            "Epoch 345: Reconstruction Loss = 0.009552\n",
            "Epoch 346: Reconstruction Loss = 0.009475\n",
            "Epoch 347: Reconstruction Loss = 0.009377\n",
            "Epoch 348: Reconstruction Loss = 0.009299\n",
            "Epoch 349: Reconstruction Loss = 0.009217\n",
            "Epoch 350: Reconstruction Loss = 0.009133\n",
            "Epoch 351: Reconstruction Loss = 0.009049\n",
            "Epoch 352: Reconstruction Loss = 0.008979\n",
            "Epoch 353: Reconstruction Loss = 0.008893\n",
            "Epoch 354: Reconstruction Loss = 0.008816\n",
            "Epoch 355: Reconstruction Loss = 0.008739\n",
            "Epoch 356: Reconstruction Loss = 0.008660\n",
            "Epoch 357: Reconstruction Loss = 0.008584\n",
            "Epoch 358: Reconstruction Loss = 0.008499\n",
            "Epoch 359: Reconstruction Loss = 0.008430\n",
            "Epoch 360: Reconstruction Loss = 0.008355\n",
            "Epoch 361: Reconstruction Loss = 0.008285\n",
            "Epoch 362: Reconstruction Loss = 0.008212\n",
            "Epoch 363: Reconstruction Loss = 0.008143\n",
            "Epoch 364: Reconstruction Loss = 0.008064\n",
            "Epoch 365: Reconstruction Loss = 0.007998\n",
            "Epoch 366: Reconstruction Loss = 0.007931\n",
            "Epoch 367: Reconstruction Loss = 0.007860\n",
            "Epoch 368: Reconstruction Loss = 0.007787\n",
            "Epoch 369: Reconstruction Loss = 0.007721\n",
            "Epoch 370: Reconstruction Loss = 0.007660\n",
            "Epoch 371: Reconstruction Loss = 0.007591\n",
            "Epoch 372: Reconstruction Loss = 0.007529\n",
            "Epoch 373: Reconstruction Loss = 0.007463\n",
            "Epoch 374: Reconstruction Loss = 0.007401\n",
            "Epoch 375: Reconstruction Loss = 0.007337\n",
            "Epoch 376: Reconstruction Loss = 0.007274\n",
            "Epoch 377: Reconstruction Loss = 0.007214\n",
            "Epoch 378: Reconstruction Loss = 0.007156\n",
            "Epoch 379: Reconstruction Loss = 0.007092\n",
            "Epoch 380: Reconstruction Loss = 0.007031\n",
            "Epoch 381: Reconstruction Loss = 0.006975\n",
            "Epoch 382: Reconstruction Loss = 0.006919\n",
            "Epoch 383: Reconstruction Loss = 0.006854\n",
            "Epoch 384: Reconstruction Loss = 0.006791\n",
            "Epoch 385: Reconstruction Loss = 0.006739\n",
            "Epoch 386: Reconstruction Loss = 0.006683\n",
            "Epoch 387: Reconstruction Loss = 0.006628\n",
            "Epoch 388: Reconstruction Loss = 0.006570\n",
            "Epoch 389: Reconstruction Loss = 0.006524\n",
            "Epoch 390: Reconstruction Loss = 0.006465\n",
            "Epoch 391: Reconstruction Loss = 0.006414\n",
            "Epoch 392: Reconstruction Loss = 0.006359\n",
            "Epoch 393: Reconstruction Loss = 0.006307\n",
            "Epoch 394: Reconstruction Loss = 0.006259\n",
            "Epoch 395: Reconstruction Loss = 0.006210\n",
            "Epoch 396: Reconstruction Loss = 0.006162\n",
            "Epoch 397: Reconstruction Loss = 0.006112\n",
            "Epoch 398: Reconstruction Loss = 0.006069\n",
            "Epoch 399: Reconstruction Loss = 0.006017\n",
            "Epoch 400: Reconstruction Loss = 0.005970\n",
            "Epoch 401: Reconstruction Loss = 0.005925\n",
            "Epoch 402: Reconstruction Loss = 0.005882\n",
            "Epoch 403: Reconstruction Loss = 0.005837\n",
            "Epoch 404: Reconstruction Loss = 0.005790\n",
            "Epoch 405: Reconstruction Loss = 0.005749\n",
            "Epoch 406: Reconstruction Loss = 0.005704\n",
            "Epoch 407: Reconstruction Loss = 0.005661\n",
            "Epoch 408: Reconstruction Loss = 0.005622\n",
            "Epoch 409: Reconstruction Loss = 0.005581\n",
            "Epoch 410: Reconstruction Loss = 0.005544\n",
            "Epoch 411: Reconstruction Loss = 0.005505\n",
            "Epoch 412: Reconstruction Loss = 0.005467\n",
            "Epoch 413: Reconstruction Loss = 0.005428\n",
            "Epoch 414: Reconstruction Loss = 0.005390\n",
            "Epoch 415: Reconstruction Loss = 0.005351\n",
            "Epoch 416: Reconstruction Loss = 0.005318\n",
            "Epoch 417: Reconstruction Loss = 0.005286\n",
            "Epoch 418: Reconstruction Loss = 0.005246\n",
            "Epoch 419: Reconstruction Loss = 0.005209\n",
            "Epoch 420: Reconstruction Loss = 0.005178\n",
            "Epoch 421: Reconstruction Loss = 0.005142\n",
            "Epoch 422: Reconstruction Loss = 0.005112\n",
            "Epoch 423: Reconstruction Loss = 0.005078\n",
            "Epoch 424: Reconstruction Loss = 0.005045\n",
            "Epoch 425: Reconstruction Loss = 0.005017\n",
            "Epoch 426: Reconstruction Loss = 0.004985\n",
            "Epoch 427: Reconstruction Loss = 0.004949\n",
            "Epoch 428: Reconstruction Loss = 0.004921\n",
            "Epoch 429: Reconstruction Loss = 0.004889\n",
            "Epoch 430: Reconstruction Loss = 0.004864\n",
            "Epoch 431: Reconstruction Loss = 0.004834\n",
            "Epoch 432: Reconstruction Loss = 0.004808\n",
            "Epoch 433: Reconstruction Loss = 0.004779\n",
            "Epoch 434: Reconstruction Loss = 0.004752\n",
            "Epoch 435: Reconstruction Loss = 0.004724\n",
            "Epoch 436: Reconstruction Loss = 0.004701\n",
            "Epoch 437: Reconstruction Loss = 0.004674\n",
            "Epoch 438: Reconstruction Loss = 0.004648\n",
            "Epoch 439: Reconstruction Loss = 0.004621\n",
            "Epoch 440: Reconstruction Loss = 0.004598\n",
            "Epoch 441: Reconstruction Loss = 0.004571\n",
            "Epoch 442: Reconstruction Loss = 0.004548\n",
            "Epoch 443: Reconstruction Loss = 0.004526\n",
            "Epoch 444: Reconstruction Loss = 0.004502\n",
            "Epoch 445: Reconstruction Loss = 0.004476\n",
            "Epoch 446: Reconstruction Loss = 0.004456\n",
            "Epoch 447: Reconstruction Loss = 0.004431\n",
            "Epoch 448: Reconstruction Loss = 0.004409\n",
            "Epoch 449: Reconstruction Loss = 0.004390\n",
            "Epoch 450: Reconstruction Loss = 0.004367\n",
            "Epoch 451: Reconstruction Loss = 0.004346\n",
            "Epoch 452: Reconstruction Loss = 0.004324\n",
            "Epoch 453: Reconstruction Loss = 0.004304\n",
            "Epoch 454: Reconstruction Loss = 0.004284\n",
            "Epoch 455: Reconstruction Loss = 0.004267\n",
            "Epoch 456: Reconstruction Loss = 0.004245\n",
            "Epoch 457: Reconstruction Loss = 0.004227\n",
            "Epoch 458: Reconstruction Loss = 0.004208\n",
            "Epoch 459: Reconstruction Loss = 0.004186\n",
            "Epoch 460: Reconstruction Loss = 0.004167\n",
            "Epoch 461: Reconstruction Loss = 0.004153\n",
            "Epoch 462: Reconstruction Loss = 0.004138\n",
            "Epoch 463: Reconstruction Loss = 0.004115\n",
            "Epoch 464: Reconstruction Loss = 0.004094\n",
            "Epoch 465: Reconstruction Loss = 0.004078\n",
            "Epoch 466: Reconstruction Loss = 0.004062\n",
            "Epoch 467: Reconstruction Loss = 0.004042\n",
            "Epoch 468: Reconstruction Loss = 0.004024\n",
            "Epoch 469: Reconstruction Loss = 0.004009\n",
            "Epoch 470: Reconstruction Loss = 0.003990\n",
            "Epoch 471: Reconstruction Loss = 0.003975\n",
            "Epoch 472: Reconstruction Loss = 0.003959\n",
            "Epoch 473: Reconstruction Loss = 0.003942\n",
            "Epoch 474: Reconstruction Loss = 0.003923\n",
            "Epoch 475: Reconstruction Loss = 0.003908\n",
            "Epoch 476: Reconstruction Loss = 0.003893\n",
            "Epoch 477: Reconstruction Loss = 0.003878\n",
            "Epoch 478: Reconstruction Loss = 0.003861\n",
            "Epoch 479: Reconstruction Loss = 0.003851\n",
            "Epoch 480: Reconstruction Loss = 0.003832\n",
            "Epoch 481: Reconstruction Loss = 0.003817\n",
            "Epoch 482: Reconstruction Loss = 0.003803\n",
            "Epoch 483: Reconstruction Loss = 0.003787\n",
            "Epoch 484: Reconstruction Loss = 0.003775\n",
            "Epoch 485: Reconstruction Loss = 0.003761\n",
            "Epoch 486: Reconstruction Loss = 0.003748\n",
            "Epoch 487: Reconstruction Loss = 0.003731\n",
            "Epoch 488: Reconstruction Loss = 0.003718\n",
            "Epoch 489: Reconstruction Loss = 0.003705\n",
            "Epoch 490: Reconstruction Loss = 0.003690\n",
            "Epoch 491: Reconstruction Loss = 0.003677\n",
            "Epoch 492: Reconstruction Loss = 0.003665\n",
            "Epoch 493: Reconstruction Loss = 0.003651\n",
            "Epoch 494: Reconstruction Loss = 0.003638\n",
            "Epoch 495: Reconstruction Loss = 0.003624\n",
            "Epoch 496: Reconstruction Loss = 0.003614\n",
            "Epoch 497: Reconstruction Loss = 0.003600\n",
            "Epoch 498: Reconstruction Loss = 0.003584\n",
            "Epoch 499: Reconstruction Loss = 0.003574\n",
            "Epoch 500: Reconstruction Loss = 0.003561\n",
            "Epoch 501: Reconstruction Loss = 0.003548\n",
            "Epoch 502: Reconstruction Loss = 0.003539\n",
            "Epoch 503: Reconstruction Loss = 0.003524\n",
            "Epoch 504: Reconstruction Loss = 0.003515\n",
            "Epoch 505: Reconstruction Loss = 0.003500\n",
            "Epoch 506: Reconstruction Loss = 0.003488\n",
            "Epoch 507: Reconstruction Loss = 0.003479\n",
            "Epoch 508: Reconstruction Loss = 0.003466\n",
            "Epoch 509: Reconstruction Loss = 0.003454\n",
            "Epoch 510: Reconstruction Loss = 0.003445\n",
            "Epoch 511: Reconstruction Loss = 0.003433\n",
            "Epoch 512: Reconstruction Loss = 0.003422\n",
            "Epoch 513: Reconstruction Loss = 0.003414\n",
            "Epoch 514: Reconstruction Loss = 0.003400\n",
            "Epoch 515: Reconstruction Loss = 0.003391\n",
            "Epoch 516: Reconstruction Loss = 0.003381\n",
            "Epoch 517: Reconstruction Loss = 0.003368\n",
            "Epoch 518: Reconstruction Loss = 0.003356\n",
            "Epoch 519: Reconstruction Loss = 0.003348\n",
            "Epoch 520: Reconstruction Loss = 0.003341\n",
            "Epoch 521: Reconstruction Loss = 0.003327\n",
            "Epoch 522: Reconstruction Loss = 0.003313\n",
            "Epoch 523: Reconstruction Loss = 0.003307\n",
            "Epoch 524: Reconstruction Loss = 0.003302\n",
            "Epoch 525: Reconstruction Loss = 0.003285\n",
            "Epoch 526: Reconstruction Loss = 0.003273\n",
            "Epoch 527: Reconstruction Loss = 0.003266\n",
            "Epoch 528: Reconstruction Loss = 0.003257\n",
            "Epoch 529: Reconstruction Loss = 0.003247\n",
            "Epoch 530: Reconstruction Loss = 0.003234\n",
            "Epoch 531: Reconstruction Loss = 0.003228\n",
            "Epoch 532: Reconstruction Loss = 0.003219\n",
            "Epoch 533: Reconstruction Loss = 0.003205\n",
            "Epoch 534: Reconstruction Loss = 0.003198\n",
            "Epoch 535: Reconstruction Loss = 0.003191\n",
            "Epoch 536: Reconstruction Loss = 0.003178\n",
            "Epoch 537: Reconstruction Loss = 0.003167\n",
            "Epoch 538: Reconstruction Loss = 0.003161\n",
            "Epoch 539: Reconstruction Loss = 0.003149\n",
            "Epoch 540: Reconstruction Loss = 0.003140\n",
            "Epoch 541: Reconstruction Loss = 0.003133\n",
            "Epoch 542: Reconstruction Loss = 0.003123\n",
            "Epoch 543: Reconstruction Loss = 0.003112\n",
            "Epoch 544: Reconstruction Loss = 0.003106\n",
            "Epoch 545: Reconstruction Loss = 0.003099\n",
            "Epoch 546: Reconstruction Loss = 0.003088\n",
            "Epoch 547: Reconstruction Loss = 0.003079\n",
            "Epoch 548: Reconstruction Loss = 0.003069\n",
            "Epoch 549: Reconstruction Loss = 0.003060\n",
            "Epoch 550: Reconstruction Loss = 0.003052\n",
            "Epoch 551: Reconstruction Loss = 0.003044\n",
            "Epoch 552: Reconstruction Loss = 0.003034\n",
            "Epoch 553: Reconstruction Loss = 0.003026\n",
            "Epoch 554: Reconstruction Loss = 0.003020\n",
            "Epoch 555: Reconstruction Loss = 0.003011\n",
            "Epoch 556: Reconstruction Loss = 0.003002\n",
            "Epoch 557: Reconstruction Loss = 0.002995\n",
            "Epoch 558: Reconstruction Loss = 0.002987\n",
            "Epoch 559: Reconstruction Loss = 0.002979\n",
            "Epoch 560: Reconstruction Loss = 0.002970\n",
            "Epoch 561: Reconstruction Loss = 0.002962\n",
            "Epoch 562: Reconstruction Loss = 0.002954\n",
            "Epoch 563: Reconstruction Loss = 0.002945\n",
            "Epoch 564: Reconstruction Loss = 0.002938\n",
            "Epoch 565: Reconstruction Loss = 0.002930\n",
            "Epoch 566: Reconstruction Loss = 0.002924\n",
            "Epoch 567: Reconstruction Loss = 0.002916\n",
            "Epoch 568: Reconstruction Loss = 0.002908\n",
            "Epoch 569: Reconstruction Loss = 0.002901\n",
            "Epoch 570: Reconstruction Loss = 0.002893\n",
            "Epoch 571: Reconstruction Loss = 0.002885\n",
            "Epoch 572: Reconstruction Loss = 0.002879\n",
            "Epoch 573: Reconstruction Loss = 0.002871\n",
            "Epoch 574: Reconstruction Loss = 0.002863\n",
            "Epoch 575: Reconstruction Loss = 0.002856\n",
            "Epoch 576: Reconstruction Loss = 0.002849\n",
            "Epoch 577: Reconstruction Loss = 0.002841\n",
            "Epoch 578: Reconstruction Loss = 0.002834\n",
            "Epoch 579: Reconstruction Loss = 0.002827\n",
            "Epoch 580: Reconstruction Loss = 0.002820\n",
            "Epoch 581: Reconstruction Loss = 0.002813\n",
            "Epoch 582: Reconstruction Loss = 0.002806\n",
            "Epoch 583: Reconstruction Loss = 0.002799\n",
            "Epoch 584: Reconstruction Loss = 0.002793\n",
            "Epoch 585: Reconstruction Loss = 0.002785\n",
            "Epoch 586: Reconstruction Loss = 0.002781\n",
            "Epoch 587: Reconstruction Loss = 0.002774\n",
            "Epoch 588: Reconstruction Loss = 0.002770\n",
            "Epoch 589: Reconstruction Loss = 0.002760\n",
            "Epoch 590: Reconstruction Loss = 0.002751\n",
            "Epoch 591: Reconstruction Loss = 0.002746\n",
            "Epoch 592: Reconstruction Loss = 0.002740\n",
            "Epoch 593: Reconstruction Loss = 0.002731\n",
            "Epoch 594: Reconstruction Loss = 0.002726\n",
            "Epoch 595: Reconstruction Loss = 0.002723\n",
            "Epoch 596: Reconstruction Loss = 0.002717\n",
            "Epoch 597: Reconstruction Loss = 0.002705\n",
            "Epoch 598: Reconstruction Loss = 0.002699\n",
            "Epoch 599: Reconstruction Loss = 0.002694\n",
            "Epoch 600: Reconstruction Loss = 0.002688\n",
            "Epoch 601: Reconstruction Loss = 0.002679\n",
            "Epoch 602: Reconstruction Loss = 0.002675\n",
            "Epoch 603: Reconstruction Loss = 0.002674\n",
            "Epoch 604: Reconstruction Loss = 0.002661\n",
            "Epoch 605: Reconstruction Loss = 0.002654\n",
            "Epoch 606: Reconstruction Loss = 0.002652\n",
            "Epoch 607: Reconstruction Loss = 0.002649\n",
            "Epoch 608: Reconstruction Loss = 0.002638\n",
            "Epoch 609: Reconstruction Loss = 0.002629\n",
            "Epoch 610: Reconstruction Loss = 0.002627\n",
            "Epoch 611: Reconstruction Loss = 0.002619\n",
            "Epoch 612: Reconstruction Loss = 0.002611\n",
            "Epoch 613: Reconstruction Loss = 0.002606\n",
            "Epoch 614: Reconstruction Loss = 0.002599\n",
            "Epoch 615: Reconstruction Loss = 0.002593\n",
            "Epoch 616: Reconstruction Loss = 0.002589\n",
            "Epoch 617: Reconstruction Loss = 0.002586\n",
            "Epoch 618: Reconstruction Loss = 0.002579\n",
            "Epoch 619: Reconstruction Loss = 0.002570\n",
            "Epoch 620: Reconstruction Loss = 0.002565\n",
            "Epoch 621: Reconstruction Loss = 0.002561\n",
            "Epoch 622: Reconstruction Loss = 0.002553\n",
            "Epoch 623: Reconstruction Loss = 0.002548\n",
            "Epoch 624: Reconstruction Loss = 0.002542\n",
            "Epoch 625: Reconstruction Loss = 0.002538\n",
            "Epoch 626: Reconstruction Loss = 0.002531\n",
            "Epoch 627: Reconstruction Loss = 0.002525\n",
            "Epoch 628: Reconstruction Loss = 0.002519\n",
            "Epoch 629: Reconstruction Loss = 0.002515\n",
            "Epoch 630: Reconstruction Loss = 0.002509\n",
            "Epoch 631: Reconstruction Loss = 0.002505\n",
            "Epoch 632: Reconstruction Loss = 0.002496\n",
            "Epoch 633: Reconstruction Loss = 0.002493\n",
            "Epoch 634: Reconstruction Loss = 0.002487\n",
            "Epoch 635: Reconstruction Loss = 0.002482\n",
            "Epoch 636: Reconstruction Loss = 0.002475\n",
            "Epoch 637: Reconstruction Loss = 0.002470\n",
            "Epoch 638: Reconstruction Loss = 0.002465\n",
            "Epoch 639: Reconstruction Loss = 0.002457\n",
            "Epoch 640: Reconstruction Loss = 0.002455\n",
            "Epoch 641: Reconstruction Loss = 0.002449\n",
            "Epoch 642: Reconstruction Loss = 0.002443\n",
            "Epoch 643: Reconstruction Loss = 0.002438\n",
            "Epoch 644: Reconstruction Loss = 0.002433\n",
            "Epoch 645: Reconstruction Loss = 0.002427\n",
            "Epoch 646: Reconstruction Loss = 0.002422\n",
            "Epoch 647: Reconstruction Loss = 0.002418\n",
            "Epoch 648: Reconstruction Loss = 0.002411\n",
            "Epoch 649: Reconstruction Loss = 0.002407\n",
            "Epoch 650: Reconstruction Loss = 0.002401\n",
            "Epoch 651: Reconstruction Loss = 0.002395\n",
            "Epoch 652: Reconstruction Loss = 0.002391\n",
            "Epoch 653: Reconstruction Loss = 0.002387\n",
            "Epoch 654: Reconstruction Loss = 0.002384\n",
            "Epoch 655: Reconstruction Loss = 0.002376\n",
            "Epoch 656: Reconstruction Loss = 0.002372\n",
            "Epoch 657: Reconstruction Loss = 0.002368\n",
            "Epoch 658: Reconstruction Loss = 0.002361\n",
            "Epoch 659: Reconstruction Loss = 0.002357\n",
            "Epoch 660: Reconstruction Loss = 0.002355\n",
            "Epoch 661: Reconstruction Loss = 0.002355\n",
            "Epoch 662: Reconstruction Loss = 0.002344\n",
            "Epoch 663: Reconstruction Loss = 0.002336\n",
            "Epoch 664: Reconstruction Loss = 0.002336\n",
            "Epoch 665: Reconstruction Loss = 0.002332\n",
            "Epoch 666: Reconstruction Loss = 0.002324\n",
            "Epoch 667: Reconstruction Loss = 0.002317\n",
            "Epoch 668: Reconstruction Loss = 0.002314\n",
            "Epoch 669: Reconstruction Loss = 0.002311\n",
            "Epoch 670: Reconstruction Loss = 0.002303\n",
            "Epoch 671: Reconstruction Loss = 0.002298\n",
            "Epoch 672: Reconstruction Loss = 0.002296\n",
            "Epoch 673: Reconstruction Loss = 0.002291\n",
            "Epoch 674: Reconstruction Loss = 0.002286\n",
            "Epoch 675: Reconstruction Loss = 0.002278\n",
            "Epoch 676: Reconstruction Loss = 0.002277\n",
            "Epoch 677: Reconstruction Loss = 0.002271\n",
            "Epoch 678: Reconstruction Loss = 0.002266\n",
            "Epoch 679: Reconstruction Loss = 0.002261\n",
            "Epoch 680: Reconstruction Loss = 0.002257\n",
            "Epoch 681: Reconstruction Loss = 0.002251\n",
            "Epoch 682: Reconstruction Loss = 0.002248\n",
            "Epoch 683: Reconstruction Loss = 0.002242\n",
            "Epoch 684: Reconstruction Loss = 0.002239\n",
            "Epoch 685: Reconstruction Loss = 0.002232\n",
            "Epoch 686: Reconstruction Loss = 0.002230\n",
            "Epoch 687: Reconstruction Loss = 0.002225\n",
            "Epoch 688: Reconstruction Loss = 0.002220\n",
            "Epoch 689: Reconstruction Loss = 0.002216\n",
            "Epoch 690: Reconstruction Loss = 0.002212\n",
            "Epoch 691: Reconstruction Loss = 0.002208\n",
            "Epoch 692: Reconstruction Loss = 0.002203\n",
            "Epoch 693: Reconstruction Loss = 0.002200\n",
            "Epoch 694: Reconstruction Loss = 0.002194\n",
            "Epoch 695: Reconstruction Loss = 0.002192\n",
            "Epoch 696: Reconstruction Loss = 0.002189\n",
            "Epoch 697: Reconstruction Loss = 0.002186\n",
            "Epoch 698: Reconstruction Loss = 0.002180\n",
            "Epoch 699: Reconstruction Loss = 0.002171\n",
            "Epoch 700: Reconstruction Loss = 0.002172\n",
            "Epoch 701: Reconstruction Loss = 0.002166\n",
            "Epoch 702: Reconstruction Loss = 0.002160\n",
            "Epoch 703: Reconstruction Loss = 0.002156\n",
            "Epoch 704: Reconstruction Loss = 0.002154\n",
            "Epoch 705: Reconstruction Loss = 0.002149\n",
            "Epoch 706: Reconstruction Loss = 0.002145\n",
            "Epoch 707: Reconstruction Loss = 0.002139\n",
            "Epoch 708: Reconstruction Loss = 0.002138\n",
            "Epoch 709: Reconstruction Loss = 0.002134\n",
            "Epoch 710: Reconstruction Loss = 0.002128\n",
            "Epoch 711: Reconstruction Loss = 0.002122\n",
            "Epoch 712: Reconstruction Loss = 0.002119\n",
            "Epoch 713: Reconstruction Loss = 0.002116\n",
            "Epoch 714: Reconstruction Loss = 0.002110\n",
            "Epoch 715: Reconstruction Loss = 0.002106\n",
            "Epoch 716: Reconstruction Loss = 0.002104\n",
            "Epoch 717: Reconstruction Loss = 0.002103\n",
            "Epoch 718: Reconstruction Loss = 0.002098\n",
            "Epoch 719: Reconstruction Loss = 0.002092\n",
            "Epoch 720: Reconstruction Loss = 0.002087\n",
            "Epoch 721: Reconstruction Loss = 0.002086\n",
            "Epoch 722: Reconstruction Loss = 0.002082\n",
            "Epoch 723: Reconstruction Loss = 0.002079\n",
            "Epoch 724: Reconstruction Loss = 0.002072\n",
            "Epoch 725: Reconstruction Loss = 0.002069\n",
            "Epoch 726: Reconstruction Loss = 0.002068\n",
            "Epoch 727: Reconstruction Loss = 0.002063\n",
            "Epoch 728: Reconstruction Loss = 0.002056\n",
            "Epoch 729: Reconstruction Loss = 0.002054\n",
            "Epoch 730: Reconstruction Loss = 0.002053\n",
            "Epoch 731: Reconstruction Loss = 0.002046\n",
            "Epoch 732: Reconstruction Loss = 0.002041\n",
            "Epoch 733: Reconstruction Loss = 0.002037\n",
            "Epoch 734: Reconstruction Loss = 0.002034\n",
            "Epoch 735: Reconstruction Loss = 0.002029\n",
            "Epoch 736: Reconstruction Loss = 0.002025\n",
            "Epoch 737: Reconstruction Loss = 0.002021\n",
            "Epoch 738: Reconstruction Loss = 0.002018\n",
            "Epoch 739: Reconstruction Loss = 0.002014\n",
            "Epoch 740: Reconstruction Loss = 0.002011\n",
            "Epoch 741: Reconstruction Loss = 0.002010\n",
            "Epoch 742: Reconstruction Loss = 0.002005\n",
            "Epoch 743: Reconstruction Loss = 0.001999\n",
            "Epoch 744: Reconstruction Loss = 0.001996\n",
            "Epoch 745: Reconstruction Loss = 0.001995\n",
            "Epoch 746: Reconstruction Loss = 0.001989\n",
            "Epoch 747: Reconstruction Loss = 0.001986\n",
            "Epoch 748: Reconstruction Loss = 0.001984\n",
            "Epoch 749: Reconstruction Loss = 0.001980\n",
            "Epoch 750: Reconstruction Loss = 0.001975\n",
            "Epoch 751: Reconstruction Loss = 0.001972\n",
            "Epoch 752: Reconstruction Loss = 0.001972\n",
            "Epoch 753: Reconstruction Loss = 0.001969\n",
            "Epoch 754: Reconstruction Loss = 0.001962\n",
            "Epoch 755: Reconstruction Loss = 0.001958\n",
            "Epoch 756: Reconstruction Loss = 0.001955\n",
            "Epoch 757: Reconstruction Loss = 0.001950\n",
            "Epoch 758: Reconstruction Loss = 0.001947\n",
            "Epoch 759: Reconstruction Loss = 0.001945\n",
            "Epoch 760: Reconstruction Loss = 0.001942\n",
            "Epoch 761: Reconstruction Loss = 0.001938\n",
            "Epoch 762: Reconstruction Loss = 0.001935\n",
            "Epoch 763: Reconstruction Loss = 0.001931\n",
            "Epoch 764: Reconstruction Loss = 0.001930\n",
            "Epoch 765: Reconstruction Loss = 0.001923\n",
            "Epoch 766: Reconstruction Loss = 0.001919\n",
            "Epoch 767: Reconstruction Loss = 0.001917\n",
            "Epoch 768: Reconstruction Loss = 0.001914\n",
            "Epoch 769: Reconstruction Loss = 0.001911\n",
            "Epoch 770: Reconstruction Loss = 0.001906\n",
            "Epoch 771: Reconstruction Loss = 0.001903\n",
            "Epoch 772: Reconstruction Loss = 0.001902\n",
            "Epoch 773: Reconstruction Loss = 0.001898\n",
            "Epoch 774: Reconstruction Loss = 0.001895\n",
            "Epoch 775: Reconstruction Loss = 0.001889\n",
            "Epoch 776: Reconstruction Loss = 0.001889\n",
            "Epoch 777: Reconstruction Loss = 0.001885\n",
            "Epoch 778: Reconstruction Loss = 0.001881\n",
            "Epoch 779: Reconstruction Loss = 0.001877\n",
            "Epoch 780: Reconstruction Loss = 0.001875\n",
            "Epoch 781: Reconstruction Loss = 0.001871\n",
            "Epoch 782: Reconstruction Loss = 0.001868\n",
            "Epoch 783: Reconstruction Loss = 0.001865\n",
            "Epoch 784: Reconstruction Loss = 0.001863\n",
            "Epoch 785: Reconstruction Loss = 0.001858\n",
            "Epoch 786: Reconstruction Loss = 0.001855\n",
            "Epoch 787: Reconstruction Loss = 0.001853\n",
            "Epoch 788: Reconstruction Loss = 0.001851\n",
            "Epoch 789: Reconstruction Loss = 0.001848\n",
            "Epoch 790: Reconstruction Loss = 0.001842\n",
            "Epoch 791: Reconstruction Loss = 0.001842\n",
            "Epoch 792: Reconstruction Loss = 0.001838\n",
            "Epoch 793: Reconstruction Loss = 0.001833\n",
            "Epoch 794: Reconstruction Loss = 0.001832\n",
            "Epoch 795: Reconstruction Loss = 0.001829\n",
            "Epoch 796: Reconstruction Loss = 0.001826\n",
            "Epoch 797: Reconstruction Loss = 0.001822\n",
            "Epoch 798: Reconstruction Loss = 0.001819\n",
            "Epoch 799: Reconstruction Loss = 0.001818\n",
            "Epoch 800: Reconstruction Loss = 0.001814\n",
            "Epoch 801: Reconstruction Loss = 0.001812\n",
            "Epoch 802: Reconstruction Loss = 0.001807\n",
            "Epoch 803: Reconstruction Loss = 0.001805\n",
            "Epoch 804: Reconstruction Loss = 0.001804\n",
            "Epoch 805: Reconstruction Loss = 0.001801\n",
            "Epoch 806: Reconstruction Loss = 0.001796\n",
            "Epoch 807: Reconstruction Loss = 0.001794\n",
            "Epoch 808: Reconstruction Loss = 0.001790\n",
            "Epoch 809: Reconstruction Loss = 0.001789\n",
            "Epoch 810: Reconstruction Loss = 0.001785\n",
            "Epoch 811: Reconstruction Loss = 0.001782\n",
            "Epoch 812: Reconstruction Loss = 0.001779\n",
            "Epoch 813: Reconstruction Loss = 0.001776\n",
            "Epoch 814: Reconstruction Loss = 0.001773\n",
            "Epoch 815: Reconstruction Loss = 0.001770\n",
            "Epoch 816: Reconstruction Loss = 0.001767\n",
            "Epoch 817: Reconstruction Loss = 0.001765\n",
            "Epoch 818: Reconstruction Loss = 0.001762\n",
            "Epoch 819: Reconstruction Loss = 0.001759\n",
            "Epoch 820: Reconstruction Loss = 0.001758\n",
            "Epoch 821: Reconstruction Loss = 0.001753\n",
            "Epoch 822: Reconstruction Loss = 0.001752\n",
            "Epoch 823: Reconstruction Loss = 0.001751\n",
            "Epoch 824: Reconstruction Loss = 0.001749\n",
            "Epoch 825: Reconstruction Loss = 0.001745\n",
            "Epoch 826: Reconstruction Loss = 0.001740\n",
            "Epoch 827: Reconstruction Loss = 0.001738\n",
            "Epoch 828: Reconstruction Loss = 0.001735\n",
            "Epoch 829: Reconstruction Loss = 0.001732\n",
            "Epoch 830: Reconstruction Loss = 0.001729\n",
            "Epoch 831: Reconstruction Loss = 0.001730\n",
            "Epoch 832: Reconstruction Loss = 0.001729\n",
            "Epoch 833: Reconstruction Loss = 0.001725\n",
            "Epoch 834: Reconstruction Loss = 0.001720\n",
            "Epoch 835: Reconstruction Loss = 0.001717\n",
            "Epoch 836: Reconstruction Loss = 0.001714\n",
            "Epoch 837: Reconstruction Loss = 0.001713\n",
            "Epoch 838: Reconstruction Loss = 0.001709\n",
            "Epoch 839: Reconstruction Loss = 0.001705\n",
            "Epoch 840: Reconstruction Loss = 0.001703\n",
            "Epoch 841: Reconstruction Loss = 0.001702\n",
            "Epoch 842: Reconstruction Loss = 0.001699\n",
            "Epoch 843: Reconstruction Loss = 0.001696\n",
            "Epoch 844: Reconstruction Loss = 0.001693\n",
            "Epoch 845: Reconstruction Loss = 0.001692\n",
            "Epoch 846: Reconstruction Loss = 0.001689\n",
            "Epoch 847: Reconstruction Loss = 0.001686\n",
            "Epoch 848: Reconstruction Loss = 0.001683\n",
            "Epoch 849: Reconstruction Loss = 0.001681\n",
            "Epoch 850: Reconstruction Loss = 0.001679\n",
            "Epoch 851: Reconstruction Loss = 0.001678\n",
            "Epoch 852: Reconstruction Loss = 0.001672\n",
            "Epoch 853: Reconstruction Loss = 0.001670\n",
            "Epoch 854: Reconstruction Loss = 0.001667\n",
            "Epoch 855: Reconstruction Loss = 0.001666\n",
            "Epoch 856: Reconstruction Loss = 0.001663\n",
            "Epoch 857: Reconstruction Loss = 0.001660\n",
            "Epoch 858: Reconstruction Loss = 0.001658\n",
            "Epoch 859: Reconstruction Loss = 0.001655\n",
            "Epoch 860: Reconstruction Loss = 0.001653\n",
            "Epoch 861: Reconstruction Loss = 0.001650\n",
            "Epoch 862: Reconstruction Loss = 0.001649\n",
            "Epoch 863: Reconstruction Loss = 0.001646\n",
            "Epoch 864: Reconstruction Loss = 0.001644\n",
            "Epoch 865: Reconstruction Loss = 0.001641\n",
            "Epoch 866: Reconstruction Loss = 0.001638\n",
            "Epoch 867: Reconstruction Loss = 0.001636\n",
            "Epoch 868: Reconstruction Loss = 0.001634\n",
            "Epoch 869: Reconstruction Loss = 0.001631\n",
            "Epoch 870: Reconstruction Loss = 0.001630\n",
            "Epoch 871: Reconstruction Loss = 0.001628\n",
            "Epoch 872: Reconstruction Loss = 0.001626\n",
            "Epoch 873: Reconstruction Loss = 0.001623\n",
            "Epoch 874: Reconstruction Loss = 0.001620\n",
            "Epoch 875: Reconstruction Loss = 0.001618\n",
            "Epoch 876: Reconstruction Loss = 0.001616\n",
            "Epoch 877: Reconstruction Loss = 0.001615\n",
            "Epoch 878: Reconstruction Loss = 0.001612\n",
            "Epoch 879: Reconstruction Loss = 0.001608\n",
            "Epoch 880: Reconstruction Loss = 0.001606\n",
            "Epoch 881: Reconstruction Loss = 0.001604\n",
            "Epoch 882: Reconstruction Loss = 0.001602\n",
            "Epoch 883: Reconstruction Loss = 0.001600\n",
            "Epoch 884: Reconstruction Loss = 0.001597\n",
            "Epoch 885: Reconstruction Loss = 0.001595\n",
            "Epoch 886: Reconstruction Loss = 0.001594\n",
            "Epoch 887: Reconstruction Loss = 0.001591\n",
            "Epoch 888: Reconstruction Loss = 0.001588\n",
            "Epoch 889: Reconstruction Loss = 0.001586\n",
            "Epoch 890: Reconstruction Loss = 0.001585\n",
            "Epoch 891: Reconstruction Loss = 0.001581\n",
            "Epoch 892: Reconstruction Loss = 0.001579\n",
            "Epoch 893: Reconstruction Loss = 0.001578\n",
            "Epoch 894: Reconstruction Loss = 0.001576\n",
            "Epoch 895: Reconstruction Loss = 0.001573\n",
            "Epoch 896: Reconstruction Loss = 0.001570\n",
            "Epoch 897: Reconstruction Loss = 0.001568\n",
            "Epoch 898: Reconstruction Loss = 0.001569\n",
            "Epoch 899: Reconstruction Loss = 0.001568\n",
            "Epoch 900: Reconstruction Loss = 0.001566\n",
            "Epoch 901: Reconstruction Loss = 0.001560\n",
            "Epoch 902: Reconstruction Loss = 0.001557\n",
            "Epoch 903: Reconstruction Loss = 0.001557\n",
            "Epoch 904: Reconstruction Loss = 0.001559\n",
            "Epoch 905: Reconstruction Loss = 0.001554\n",
            "Epoch 906: Reconstruction Loss = 0.001549\n",
            "Epoch 907: Reconstruction Loss = 0.001549\n",
            "Epoch 908: Reconstruction Loss = 0.001549\n",
            "Epoch 909: Reconstruction Loss = 0.001544\n",
            "Epoch 910: Reconstruction Loss = 0.001542\n",
            "Epoch 911: Reconstruction Loss = 0.001538\n",
            "Epoch 912: Reconstruction Loss = 0.001537\n",
            "Epoch 913: Reconstruction Loss = 0.001535\n",
            "Epoch 914: Reconstruction Loss = 0.001532\n",
            "Epoch 915: Reconstruction Loss = 0.001531\n",
            "Epoch 916: Reconstruction Loss = 0.001530\n",
            "Epoch 917: Reconstruction Loss = 0.001528\n",
            "Epoch 918: Reconstruction Loss = 0.001530\n",
            "Epoch 919: Reconstruction Loss = 0.001526\n",
            "Epoch 920: Reconstruction Loss = 0.001521\n",
            "Epoch 921: Reconstruction Loss = 0.001521\n",
            "Epoch 922: Reconstruction Loss = 0.001519\n",
            "Epoch 923: Reconstruction Loss = 0.001522\n",
            "Epoch 924: Reconstruction Loss = 0.001516\n",
            "Epoch 925: Reconstruction Loss = 0.001512\n",
            "Epoch 926: Reconstruction Loss = 0.001510\n",
            "Epoch 927: Reconstruction Loss = 0.001507\n",
            "Epoch 928: Reconstruction Loss = 0.001505\n",
            "Epoch 929: Reconstruction Loss = 0.001505\n",
            "Epoch 930: Reconstruction Loss = 0.001502\n",
            "Epoch 931: Reconstruction Loss = 0.001499\n",
            "Epoch 932: Reconstruction Loss = 0.001497\n",
            "Epoch 933: Reconstruction Loss = 0.001493\n",
            "Epoch 934: Reconstruction Loss = 0.001492\n",
            "Epoch 935: Reconstruction Loss = 0.001492\n",
            "Epoch 936: Reconstruction Loss = 0.001488\n",
            "Epoch 937: Reconstruction Loss = 0.001487\n",
            "Epoch 938: Reconstruction Loss = 0.001486\n",
            "Epoch 939: Reconstruction Loss = 0.001486\n",
            "Epoch 940: Reconstruction Loss = 0.001487\n",
            "Epoch 941: Reconstruction Loss = 0.001482\n",
            "Epoch 942: Reconstruction Loss = 0.001479\n",
            "Epoch 943: Reconstruction Loss = 0.001475\n",
            "Epoch 944: Reconstruction Loss = 0.001474\n",
            "Epoch 945: Reconstruction Loss = 0.001471\n",
            "Epoch 946: Reconstruction Loss = 0.001469\n",
            "Epoch 947: Reconstruction Loss = 0.001468\n",
            "Epoch 948: Reconstruction Loss = 0.001469\n",
            "Epoch 949: Reconstruction Loss = 0.001467\n",
            "Epoch 950: Reconstruction Loss = 0.001462\n",
            "Epoch 951: Reconstruction Loss = 0.001460\n",
            "Epoch 952: Reconstruction Loss = 0.001463\n",
            "Epoch 953: Reconstruction Loss = 0.001467\n",
            "Epoch 954: Reconstruction Loss = 0.001465\n",
            "Epoch 955: Reconstruction Loss = 0.001457\n",
            "Epoch 956: Reconstruction Loss = 0.001450\n",
            "Epoch 957: Reconstruction Loss = 0.001455\n",
            "Epoch 958: Reconstruction Loss = 0.001459\n",
            "Epoch 959: Reconstruction Loss = 0.001452\n",
            "Epoch 960: Reconstruction Loss = 0.001444\n",
            "Epoch 961: Reconstruction Loss = 0.001442\n",
            "Epoch 962: Reconstruction Loss = 0.001445\n",
            "Epoch 963: Reconstruction Loss = 0.001441\n",
            "Epoch 964: Reconstruction Loss = 0.001436\n",
            "Epoch 965: Reconstruction Loss = 0.001435\n",
            "Epoch 966: Reconstruction Loss = 0.001434\n",
            "Epoch 967: Reconstruction Loss = 0.001430\n",
            "Epoch 968: Reconstruction Loss = 0.001429\n",
            "Epoch 969: Reconstruction Loss = 0.001427\n",
            "Epoch 970: Reconstruction Loss = 0.001426\n",
            "Epoch 971: Reconstruction Loss = 0.001424\n",
            "Epoch 972: Reconstruction Loss = 0.001422\n",
            "Epoch 973: Reconstruction Loss = 0.001419\n",
            "Epoch 974: Reconstruction Loss = 0.001419\n",
            "Epoch 975: Reconstruction Loss = 0.001417\n",
            "Epoch 976: Reconstruction Loss = 0.001416\n",
            "Epoch 977: Reconstruction Loss = 0.001415\n",
            "Epoch 978: Reconstruction Loss = 0.001411\n",
            "Epoch 979: Reconstruction Loss = 0.001411\n",
            "Epoch 980: Reconstruction Loss = 0.001409\n",
            "Epoch 981: Reconstruction Loss = 0.001409\n",
            "Epoch 982: Reconstruction Loss = 0.001407\n",
            "Epoch 983: Reconstruction Loss = 0.001403\n",
            "Epoch 984: Reconstruction Loss = 0.001401\n",
            "Epoch 985: Reconstruction Loss = 0.001402\n",
            "Epoch 986: Reconstruction Loss = 0.001402\n",
            "Epoch 987: Reconstruction Loss = 0.001399\n",
            "Epoch 988: Reconstruction Loss = 0.001395\n",
            "Epoch 989: Reconstruction Loss = 0.001394\n",
            "Epoch 990: Reconstruction Loss = 0.001394\n",
            "Epoch 991: Reconstruction Loss = 0.001391\n",
            "Epoch 992: Reconstruction Loss = 0.001387\n",
            "Epoch 993: Reconstruction Loss = 0.001385\n",
            "Epoch 994: Reconstruction Loss = 0.001385\n",
            "Epoch 995: Reconstruction Loss = 0.001383\n",
            "Epoch 996: Reconstruction Loss = 0.001380\n",
            "Epoch 997: Reconstruction Loss = 0.001379\n",
            "Epoch 998: Reconstruction Loss = 0.001377\n",
            "Epoch 999: Reconstruction Loss = 0.001376\n",
            "Epoch 1000: Reconstruction Loss = 0.001374\n",
            "\n",
            "=== PLAN A: MISSION COMPLETE ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step11の変化をグラフ化"
      ],
      "metadata": {
        "id": "VavPqZyf9ysp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 訓練中に収集したロス値が loss_history に格納されていることを想定\n",
        "# （上記セルの実行後にこのセルを実行してください）\n",
        "\n",
        "if 'loss_history' in locals() and len(loss_history) > 0:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o', linestyle='-', markersize=4)\n",
        "    plt.title('Reconstruction Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Reconstruction Loss')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"loss_history が見つからないか、データが空です。訓練セルを先に実行してください。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "XXZYDFr1XXdl",
        "outputId": "949ab023-53f9-40cd-c616-ff2ba6a0de26"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "グラフより,Lossが着実に減少していることが読み取れた。これは、AIの脳（RSSM）が、Minigridの世界の法則を少しずつ理解し、夢の中で現実を正しくシミュレートできるようになっていることの証明である。"
      ],
      "metadata": {
        "id": "_TPASXfj90Gt"
      }
    }
  ]
}