{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1Nl2e/nh8SKiYJz2XCn91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/norim-user/Worldmodel-Report/blob/main/%E5%A4%89%E5%8C%96%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E5%86%85%E7%99%BA%E7%9A%84%E5%8B%95%E6%A9%9F%E3%82%92%E6%8C%81%E3%81%A4%E4%B8%96%E7%95%8C%E3%83%A2%E3%83%87%E3%83%AB%E3%82%A8%E3%83%BC%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%88%E3%81%AE%E5%86%8D%E7%8F%BE%E5%AE%9F%E8%A3%85(planB)_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "実装論文：変化に基づく内発的動機を持つ世界モデルエージェント,https://arxiv.org/abs/2503.21047"
      ],
      "metadata": {
        "id": "G0u_Haj_DVjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Dive: Plan B (The Strategic Path)\n",
        "「論文の新規性である『CBET（好奇心による転移学習）』の効果を手っ取り早く検証することを目的」\n",
        "\n",
        "PlanBでは、重厚な「世界モデル」の代わりに、軽快な「モデルフリー強化学習（PPOなど）」を脳として採用します。\n",
        "CBETは元々モデルフリー（IMPALA）のために開発された技術であるため。\n",
        "\n",
        "\n",
        "Stable Baselines3を使って、標準的なAI（PPO）を作ります。\n",
        "\n",
        "DualModelPolicyラッパーをPPOに適用し、転移学習ができるか実験します。\n"
      ],
      "metadata": {
        "id": "SpxkjHAIwxCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Install Dependencies\n",
        "Minigridと、可視化用のライブラリをインストールします。"
      ],
      "metadata": {
        "id": "PdBxG7_uDfpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR4blaKtDPAy",
        "outputId": "4a053736-3d03-444d-9290-13efad764f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minigrid\n",
            "  Downloading minigrid-3.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu128)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from minigrid) (1.2.3)\n",
            "Requirement already satisfied: pygame>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.6.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym) (3.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym) (0.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (0.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading minigrid-3.0.0-py3-none-any.whl (136 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: minigrid\n",
            "Successfully installed minigrid-3.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install Minigrid and dependencies\n",
        "!pip install minigrid gym matplotlib torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Verify Environment (The \"Unlock\" Task)\n",
        "論文でタスク環境として使用されている 'MiniGrid-Unlock-v0' (あるいはそれに準ずる環境) をロードし、エージェントの視界（Observation）を確認"
      ],
      "metadata": {
        "id": "-Vlgv4ZzDxY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym # Minigrid now uses gymnasium\n",
        "import minigrid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 論文で使用されている \"Unlock\" 環境 (Task Environment)\n",
        "# ※最新のMinigridでは名称が少し異なる場合がありますが、基本構造は同じです\n",
        "env_name = 'MiniGrid-Unlock-v0'\n",
        "\n",
        "try:\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "except gym.error.Error:\n",
        "    # フォールバック: 標準的な環境を使用\n",
        "    print(f\"{env_name} not found, using MiniGrid-DoorKey-5x5-v0\")\n",
        "    env = gym.make('MiniGrid-DoorKey-5x5-v0', render_mode='rgb_array')\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "# --- Visualization ---\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(env.render())\n",
        "plt.title(f\"Environment: {env.spec.id}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Check Observation Shape\n",
        "# DreamerV3は画像をそのまま入力します\n",
        "print(f\"Observation keys: {observation.keys()}\")\n",
        "print(f\"Image shape: {observation['image'].shape}\") # 通常 (7, 7, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "SqWfaiN9Dv91",
        "outputId": "ba696b29-9da3-4e16-ee18-b5dc196cb74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation keys: dict_keys(['image', 'direction', 'mission'])\n",
            "Image shape: (7, 7, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2: Implementing CBET Logic (The Intrinsic Reward)環境構築と並行して、この論文の魂である CBET (Change-Based Exploration Transfer) の核となる数式を実装しましょう。これはGPUを使わない純粋なロジックなので、Colab Freeでも実装可能です。論文の Equation 1  をコードに変換します。$$r_{i}(s)=\\frac{1}{n(s)+n(c)}$$$r_i(s)$: 好奇心報酬（Intrinsic Reward）\n",
        "\n",
        "$n(s)$: その状態 $s$ を訪れた回数（Visits count）\n",
        "\n",
        "$n(c)$: 変化 $c$ が起きた回数（Change count）\n",
        "\n",
        "$c$: 状態の変化（$s' - s$）"
      ],
      "metadata": {
        "id": "zlOvqOYBEHz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step3:CBET Module Implementation (PyTorch)"
      ],
      "metadata": {
        "id": "j9U509lUFQXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import hashlib\n",
        "\n",
        "class CBETReward:\n",
        "    def __init__(self, decay=0.99):\n",
        "        \"\"\"\n",
        "        論文に基づくCBETの実装。\n",
        "        Args:\n",
        "            decay: 論文では言及が少ないが、カウントを無限に増やさないための減衰率(任意)\n",
        "                   論文では random reset を採用している [cite: 61]\n",
        "        \"\"\"\n",
        "        self.state_counts = {}  # n(s)\n",
        "        self.change_counts = {} # n(c)\n",
        "\n",
        "    def get_state_hash(self, state_tensor):\n",
        "        \"\"\"\n",
        "        高次元の状態をハッシュ化して辞書のキーにする。\n",
        "        論文ではSimHashなどが使われるが、ここでは簡易的に文字列化またはtostringを使用。\n",
        "        Minigridのような離散的な状態ならこれで十分。\n",
        "        \"\"\"\n",
        "        # 簡易実装: tensorをバイト列にしてハッシュ化\n",
        "        return hashlib.md5(state_tensor.cpu().numpy().tobytes()).hexdigest()\n",
        "\n",
        "    def compute_reward(self, current_state, next_state):\n",
        "        \"\"\"\n",
        "        Eq. 1 に基づく報酬計算\n",
        "        \"\"\"\n",
        "        # 1. 状態のハッシュ化\n",
        "        s_hash = self.get_state_hash(current_state)\n",
        "\n",
        "        # 2. 変化(Change)の計算: c = s' - s\n",
        "        # 画像ベースの場合、ピクセル差分をとる\n",
        "        change = next_state - current_state\n",
        "        c_hash = self.get_state_hash(change)\n",
        "\n",
        "        # 3. カウント更新\n",
        "        self.state_counts[s_hash] = self.state_counts.get(s_hash, 0) + 1\n",
        "        self.change_counts[c_hash] = self.change_counts.get(c_hash, 0) + 1\n",
        "\n",
        "        n_s = self.state_counts[s_hash]\n",
        "        n_c = self.change_counts[c_hash]\n",
        "\n",
        "        # 4. Intrinsic Reward 計算\n",
        "        r_i = 1.0 / (n_s + n_c)\n",
        "\n",
        "        return r_i\n",
        "\n",
        "# --- Test the Module ---\n",
        "# ダミーデータでテスト\n",
        "cbet = CBETReward()\n",
        "state_t = torch.zeros((3, 7, 7)) # ダミー画像\n",
        "state_t1 = state_t.clone()\n",
        "state_t1[0, 1, 1] = 1.0 # 少し変化させる\n",
        "\n",
        "reward = cbet.compute_reward(state_t, state_t1)\n",
        "print(f\"Intrinsic Reward for first change: {reward}\") # 期待値: 1/(1+1) = 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRl1c4_WEDZV",
        "outputId": "c2338df1-cab8-4c67-cb99-1dfaa03dcf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intrinsic Reward for first change: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step4: The \"Dual-Doctor\" Logic (Eq. 3 Implementation)\n",
        "この論文の著者は、DreamerV3の内部構造を改造する困難さを避けるため、2つの独立したDreamerV3を並列に走らせ、その出力を混ぜるという解決策を取りました 。数式を確認しましょう：$$\\pi_{TASK}(x, a) = \\sigma(f_i(w_i(x), a) + f_e(w_e(x), a))$$これをコード（Python class）に落とし込みます。"
      ],
      "metadata": {
        "id": "frkByGktHtXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation Code (The Wrapper)\n",
        "\n",
        "どんなAIモデルであっても「2つ混ぜて動かす」ことができるようにする特製のラッパー"
      ],
      "metadata": {
        "id": "Uk5ZMnnNHtT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. The Mock Agent (ダミーの脳) ---\n",
        "# まずは「脳」の代わりになる簡単なクラスを作ります。\n",
        "# 本番ではここが「DreamerV3」に置き換わります。\n",
        "class MockDreamerAgent(nn.Module):\n",
        "    def __init__(self, action_dim):\n",
        "        super().__init__()\n",
        "        self.action_dim = action_dim\n",
        "        # ダミーの重み（ランダムな思考を出力するため）\n",
        "        self.dummy_layer = nn.Linear(10, action_dim)\n",
        "\n",
        "    def get_logits(self, observation):\n",
        "        \"\"\"\n",
        "        観察(画像)を受け取り、行動の優先度(Logits)を返す\n",
        "        \"\"\"\n",
        "        # 本来はここで画像をVAEで圧縮し、RNNで未来を予測する\n",
        "        # ここではダミーとしてランダムな数値を作成\n",
        "        batch_size = 1\n",
        "        dummy_feature = torch.randn(batch_size, 10)\n",
        "        logits = self.dummy_layer(dummy_feature)\n",
        "        return logits\n",
        "\n",
        "# --- 2. The Dual-Doctor Wrapper (論文の核心: Eq. 3) ---\n",
        "class DualModelPolicy(nn.Module):\n",
        "    def __init__(self, explorer_agent, worker_agent):\n",
        "        super().__init__()\n",
        "        self.explorer = explorer_agent # f_i (Intrinsic)\n",
        "        self.worker = worker_agent     # f_e (Extrinsic)\n",
        "\n",
        "    def get_action(self, observation):\n",
        "        # Step 1: 探索担当の脳に意見を聞く (f_i)\n",
        "        logits_i = self.explorer.get_logits(observation)\n",
        "\n",
        "        # Step 2: 仕事担当の脳に意見を聞く (f_e)\n",
        "        logits_e = self.worker.get_logits(observation)\n",
        "\n",
        "        # Step 3: 意見を統合する (Summation)\n",
        "        # Logits（確率になる前の生数値）を足し合わせるのがポイントです\n",
        "        combined_logits = logits_i + logits_e\n",
        "\n",
        "        # Step 4: 確率に変換して行動を選ぶ (Softmax & Sample)\n",
        "        probs = F.softmax(combined_logits, dim=-1)\n",
        "\n",
        "        # 確率に従ってサイコロを振る\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        return action.item(), probs\n",
        "\n",
        "# --- Test the Architecture ---\n",
        "action_dim = 7 # Minigridの行動パターン数\n",
        "agent_i = MockDreamerAgent(action_dim) # 事前学習済みの探索エージェント (想定)\n",
        "agent_e = MockDreamerAgent(action_dim) # これから育てるタスクエージェント\n",
        "\n",
        "# 合体させる\n",
        "dual_brain = DualModelPolicy(agent_i, agent_e)\n",
        "\n",
        "# テスト実行\n",
        "dummy_obs = torch.zeros(1, 7, 7, 3) # ダミー画像\n",
        "action, probabilities = dual_brain.get_action(dummy_obs)\n",
        "\n",
        "print(f\"Selected Action Index: {action}\")\n",
        "print(f\"Combined Probabilities: {probabilities.detach().numpy()}\")\n",
        "print(\"Dual-Doctor mechanism is working.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWhXFcXHHt88",
        "outputId": "e485cc83-11c3-425b-890c-8613e0865b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Action Index: 4\n",
            "Combined Probabilities: [[0.03868209 0.34273162 0.01969949 0.20544465 0.07809358 0.24319842\n",
            "  0.07215016]]\n",
            "Dual-Doctor mechanism is working.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step5以降はPlanAとPlanBで実行内容が異なる"
      ],
      "metadata": {
        "id": "O604J-2l6Akk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Install Stable Baselines3\n",
        "信頼性の高い強化学習ライブラリである Stable Baselines3 をインストールします。"
      ],
      "metadata": {
        "id": "XY2SAbBTLVzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Stable Baselines3 and dependencies for Minigrid\n",
        "!pip install stable-baselines3 shimmy>=0.2.1"
      ],
      "metadata": {
        "id": "AWpDCgeyLUw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: The \"Curiosity Wrapper\" Implementation\n",
        "Plan Bの心臓部です。GymのWrapperという機能を使って、環境からエージェントに渡される**報酬（Reward）**をハッキングします。エージェントが行動するたびに、裏でこっそり「変化への報酬（CBET）」を上乗せします。\n",
        "\n",
        "以前作成した CBETReward クラスを組み込みます。"
      ],
      "metadata": {
        "id": "dCDuS0yiLryf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from minigrid.wrappers import ImgObsWrapper\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "# 以前作成したCBETRewardクラスを再定義（念のため）\n",
        "import hashlib\n",
        "class CBETReward:\n",
        "    def __init__(self):\n",
        "        self.state_counts = {}\n",
        "        self.change_counts = {}\n",
        "\n",
        "    def get_hash(self, array):\n",
        "        # Numpy配列をハッシュ化\n",
        "        return hashlib.md5(array.tobytes()).hexdigest()\n",
        "\n",
        "    def compute(self, obs, next_obs):\n",
        "        # 1. 状態のハッシュ\n",
        "        s_hash = self.get_hash(obs)\n",
        "\n",
        "        # 2. 変化の計算 (s' - s)\n",
        "        change = next_obs.astype(np.float32) - obs.astype(np.float32)\n",
        "        c_hash = self.get_hash(change)\n",
        "\n",
        "        # 3. カウント更新\n",
        "        self.state_counts[s_hash] = self.state_counts.get(s_hash, 0) + 1\n",
        "        self.change_counts[c_hash] = self.change_counts.get(c_hash, 0) + 1\n",
        "\n",
        "        # 4. 報酬計算 (Eq. 1)\n",
        "        # ゼロ除算を防ぐため +1 しています\n",
        "        r_i = 1.0 / (np.sqrt(self.state_counts[s_hash]) + np.sqrt(self.change_counts[c_hash]))\n",
        "        return r_i\n",
        "\n",
        "# --- The Wrapper ---\n",
        "class CBETEnvWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, alpha=0.1):\n",
        "        super().__init__(env)\n",
        "        self.cbet = CBETReward()\n",
        "        self.last_obs = None\n",
        "        self.alpha = alpha # Intrinsic Rewardの強さ (Eq. 4)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.last_obs = obs\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # CBET報酬の計算\n",
        "        intrinsic_reward = 0.0\n",
        "        if self.last_obs is not None:\n",
        "            intrinsic_reward = self.cbet.compute(self.last_obs, next_obs)\n",
        "\n",
        "        self.last_obs = next_obs\n",
        "\n",
        "        # 報酬の合成 (Eq. 4: r_t = r_e + alpha * r_i)\n",
        "        total_reward = reward + (self.alpha * intrinsic_reward)\n",
        "\n",
        "        # ログ用に情報を追加（あとでグラフにするため）\n",
        "        info[\"intrinsic_reward\"] = intrinsic_reward\n",
        "        info[\"extrinsic_reward\"] = reward\n",
        "\n",
        "        return next_obs, total_reward, terminated, truncated, info\n",
        "\n",
        "print(\"CBET Wrapper defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je-fUS3uLgy0",
        "outputId": "06dbcbae-cc11-46a1-9bf9-421e792573e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBET Wrapper defined successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7: Training the Agent (Tabula Rasa)\n",
        "実際にAI（PPO）の学習を実行。 比較実験として、「CBETありのエージェント」を育てます。Minigridの Unlock タスクは探索が難しいため、通常のエージェントだと学習が進まないことがありますが、好奇心を持たせることでどう変わるかを確認します。"
      ],
      "metadata": {
        "id": "qeRWQz8xL4X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper\n",
        "# RGBImgPartialObsWrapper: 7x7のデータを、ピクセル画素の画像（例: 56x56など）に変換する\n",
        "# ImgObsWrapper: 辞書データから画像部分だけを取り出す\n",
        "\n",
        "def make_env(with_cbet=True):\n",
        "    # 1. 基本環境のロード\n",
        "    env = gym.make('MiniGrid-Unlock-v0', render_mode='rgb_array')\n",
        "\n",
        "    # --- HERE IS THE FIX ---\n",
        "    # 2. \"虫眼鏡\"を適用 (7x7 grid -> Pixel Image)\n",
        "    # これにより入力サイズが (56, 56, 3) などに拡大され、CNNが食べられるサイズになります\n",
        "    env = RGBImgPartialObsWrapper(env)\n",
        "\n",
        "    # 3. 画像データのみを抽出\n",
        "    env = ImgObsWrapper(env)\n",
        "    # -----------------------\n",
        "\n",
        "    # 4. CBET (好奇心) の適用\n",
        "    if with_cbet:\n",
        "        env = CBETEnvWrapper(env, alpha=1.0)\n",
        "\n",
        "    return Monitor(env)\n",
        "\n",
        "# --- Re-Initialize Model with New Environment ---\n",
        "# 環境が変わったので、モデルも作り直す必要があります\n",
        "env = DummyVecEnv([lambda: make_env(with_cbet=True)])\n",
        "\n",
        "# 入力サイズが変わったか確認 (デバッグ用)\n",
        "print(f\"New observation shape: {env.observation_space.shape}\")\n",
        "\n",
        "model = PPO(\"CnnPolicy\", env, verbose=1, learning_rate=0.0003)\n",
        "\n",
        "print(\"Training started... (Retrying)\")\n",
        "model.learn(total_timesteps=20000)\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# 4. テスト実行\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "for _ in range(100):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(f\"Test Run Reward: {total_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c20OIXdLoUW",
        "outputId": "b83c2281-9692-4685-8784-05486180a847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New observation shape: (56, 56, 3)\n",
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "Training started... (Retrying)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 276      |\n",
            "|    ep_rew_mean     | 36.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 284      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 7        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 282         |\n",
            "|    ep_rew_mean          | 38          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 277         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013262732 |\n",
            "|    clip_fraction        | 0.205       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | 0.00713     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.253       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0233     |\n",
            "|    value_loss           | 0.605       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 274       |\n",
            "|    ep_rew_mean          | 37.7      |\n",
            "| time/                   |           |\n",
            "|    fps                  | 279       |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 21        |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0151664 |\n",
            "|    clip_fraction        | 0.282     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.89     |\n",
            "|    explained_variance   | 0.00217   |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.255     |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -0.0315   |\n",
            "|    value_loss           | 0.674     |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 278         |\n",
            "|    ep_rew_mean          | 41.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 278         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016545754 |\n",
            "|    clip_fraction        | 0.298       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.84       |\n",
            "|    explained_variance   | -0.00118    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.324       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0305     |\n",
            "|    value_loss           | 0.85        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 280         |\n",
            "|    ep_rew_mean          | 42.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 283         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 36          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016354473 |\n",
            "|    clip_fraction        | 0.295       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | -0.0164     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.193       |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0223     |\n",
            "|    value_loss           | 0.626       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 281        |\n",
            "|    ep_rew_mean          | 43.9       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 283        |\n",
            "|    iterations           | 6          |\n",
            "|    time_elapsed         | 43         |\n",
            "|    total_timesteps      | 12288      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01712725 |\n",
            "|    clip_fraction        | 0.29       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.7       |\n",
            "|    explained_variance   | 0.058      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.288      |\n",
            "|    n_updates            | 50         |\n",
            "|    policy_gradient_loss | -0.0178    |\n",
            "|    value_loss           | 0.592      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 282         |\n",
            "|    ep_rew_mean          | 46          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 50          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015643256 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.63       |\n",
            "|    explained_variance   | -0.154      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.352       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0121     |\n",
            "|    value_loss           | 0.94        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 283         |\n",
            "|    ep_rew_mean          | 47.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 57          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016685573 |\n",
            "|    clip_fraction        | 0.23        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.56       |\n",
            "|    explained_variance   | -0.0627     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.331       |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    value_loss           | 0.761       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 283         |\n",
            "|    ep_rew_mean          | 49          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 64          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016715148 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.187       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.304       |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0108     |\n",
            "|    value_loss           | 0.804       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 284         |\n",
            "|    ep_rew_mean          | 50          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 286         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 71          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011519752 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.218       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.347       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00673    |\n",
            "|    value_loss           | 0.664       |\n",
            "-----------------------------------------\n",
            "Training finished.\n",
            "Test Run Reward: [22.90654]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習ログの ep_rew_mean（平均報酬）が 40.9 から 48.9 に上昇したことが確認できた。これは、エージェントが「未知の状態（変化）」を発見し、そこから好奇心報酬（Intrinsic Reward）を得て成長した証拠です。このことから好奇心によって報酬が増えることは確認できた。"
      ],
      "metadata": {
        "id": "cjVt-s2OGCMr"
      }
    }
  ]
}